<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Intrusion Detection Systems (X-IDS): A Survey of Current Methods, Challenges, and Opportunities</title>
				<funder>
					<orgName type="full">Mississippi State University</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Department of Defense (DoD) High Performance Computing Modernization Program</orgName>
				</funder>
				<funder ref="#_jC4NGbF">
					<orgName type="full">U.S. Army Engineering Research and Develop Center</orgName>
					<orgName type="abbreviated">ERDC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Subash</forename><surname>Neupane</surname></persName>
							<idno type="ORCID">0000-0001-9260-3914</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Ables</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Graduat Student Member, IEEE</roleName><forename type="first">William</forename><surname>Anders</surname></persName>
							<idno type="ORCID">0000-0001-5206-1436</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sudip</forename><surname>Mittal</surname></persName>
							<idno type="ORCID">0000-0001-9151-8347</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shahram</forename><surname>Rahimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ioana</forename><surname>Banicescu</surname></persName>
							<idno type="ORCID">0000-0001-5206-1436</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Mississippi</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Life Senior Member, IEEE</roleName><forename type="first">Maria</forename><surname>Seale</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">U.S. Army Engineer Research and Development Center</orgName>
								<address>
									<postCode>39180</postCode>
									<settlement>Vicksburg</settlement>
									<region>Mississippi MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Intrusion Detection Systems (X-IDS): A Survey of Current Methods, Challenges, and Opportunities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9862BD167BBECB663C01EE1FBCE52E21</idno>
					<idno type="DOI">10.1109/ACCESS.2022.3216617</idno>
					<note type="submission">Received 24 August 2022, accepted 17 October 2022, date of publication 25 October 2022, date of current version 31 October 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-02T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable intrusion detection systems</term>
					<term>explainable artificial intelligence</term>
					<term>machine learning</term>
					<term>deep learning</term>
					<term>white box</term>
					<term>black box</term>
					<term>explainability</term>
					<term>cybersecurity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The application of Artificial Intelligence (AI) and Machine Learning (ML) to cybersecurity challenges has gained traction in industry and academia, partially as a result of widespread malware attacks on critical systems such as cloud infrastructures and government institutions. Intrusion Detection Systems (IDS), using some forms of AI, have received widespread adoption due to their ability to handle vast amounts of data with a high prediction accuracy. These systems are hosted in the organizational Cyber Security Operation Center (CSoC) as a defense tool to monitor and detect malicious network flow that would otherwise impact the Confidentiality, Integrity, and Availability (CIA). CSoC analysts rely on these systems to make decisions about the detected threats. However, IDSs designed using Deep Learning (DL) techniques are often treated as black box models and do not provide a justification for their predictions. This creates a barrier for CSoC analysts, as they are unable to improve their decisions based on the model's predictions. One solution to this problem is to design explainable IDS (X-IDS). This survey reviews the state-of-the-art in explainable AI (XAI) for IDS, its current challenges, and discusses how these challenges span to the design of an X-IDS. In particular, we discuss black box and white box approaches comprehensively. We also present the tradeoff between these approaches in terms of their performance and ability to produce explanations. Furthermore, we propose a generic architecture that considers human-in-the-loop which can be used as a guideline when designing an X-IDS. Research recommendations are given from three critical viewpoints: the need to define explainability for IDS, the need to create explanations tailored to various stakeholders, and the need to design metrics to evaluate explanations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND MOTIVATION</head><p>The use of Artificial Intelligence (AI) and Machine Learning (ML) to solve cybersecurity problems has been gaining traction within industry and academia, partly as a response to widespread malware attacks on critical systems, such as cloud infrastructures, government institutions, etc. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>,</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Mansoor Ahmed . <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. AI-and ML-assisted cybersecurity offers data-driven automation that could enable security systems to identify and respond to cyber threats in real time. Many of these AI-based cyber defense systems are hosted in an organizational Cyber Security Operations Center (CSoC). CSoCs operated by security analysts act as a cybersecurity information hub and a defense base. Here the task is to orchestrate different security systems that are a part of an organization's overall cybersecurity framework, many of which have AI components. Examples of these security systems include Security Information and Event Management (SIEM) systems, vulnerability assessment solutions, governance, risk and compliance systems, application and database scanners, Intrusion Detection Systems (IDS), user and entity behavior analytics, Endpoint Detection and Remediation (EDR), etc. Here the security analysts maintain an ''organizational state'', keeping themselves one step ahead of the attackers to prevent potential intrusions <ref type="bibr" target="#b11">[12]</ref>.</p><p>The term ''intrusion detection'' originated in the early 1980s with James Anderson's seminal paper <ref type="bibr" target="#b12">[13]</ref>. Dorothy E. Denning <ref type="bibr" target="#b13">[14]</ref>, following Anderson's work, proposed the first functional IDS in the mid-1980s. An IDS is a software or hardware security system that automates the process of monitoring and analyzing events occurring within a computer system or network for indications of potential security problems before they inflict widespread damage <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>In general, an intrusion results in a breach of at least one of the principles: Confidentiality, Integrity, or Availability (CIA). These tenets of security are used when protecting modern data infrastructure. They refer to the permissions to access or modify data, the prevention of improper data modification, and the ability to access data. The objective of an IDS is to detect misuse, unauthorized use (outsider without authorization), and abuse (abusing privilege-e.g., insider threat) within an organization, and much research has been done to improve the operational capacity of these IDS <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and <ref type="bibr" target="#b18">[19]</ref>.</p><p>The literature shows that numerous IDSs have been developed through the application of a variety of techniques from an array of disciplines, including statistical methods, ML techniques, and others <ref type="bibr">[20]</ref>. At present, ML and Deep Learning (DL) techniques are widely used to develop IDS because of their ability to attain a high detection rate <ref type="bibr" target="#b15">[16]</ref>. This adoption is also attributed to the fact that IDSs based on ML/DL techniques are much more efficient, accurate, and extendable as compared to their counterparts developed using other techniques. The surveys in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and <ref type="bibr" target="#b21">[22]</ref> primarily focus on intrusion detection techniques based on deep learning. However, the techniques described in the preceding surveys are deficient in their ability to explain their inference processes and final results, and they are frequently treated as a black box by both developers and users <ref type="bibr" target="#b22">[23]</ref>. As a result, there is growing concern about the possibility of bias in these models, which necessitates the requirements for model transparency and post-hoc explainability <ref type="bibr" target="#b23">[24]</ref>. Unfortunately, the majority of black box IDS described in the literature are opaque and much is needed to augment transparency.</p><p>It is apparent that these opaque/non-transparent models can achieve impressive prediction accuracies; however, they lack justification for their predictions. This is due to their nested and non-linear structure, which makes it difficult to identify the precise information in the data that influences their decision-making <ref type="bibr" target="#b24">[25]</ref>. Such a lack of understanding about the inner workings of opaque AI models or an inability to traverse back from the outputs to the original data raises user trust issues <ref type="bibr" target="#b25">[26]</ref>. This black box nature of the models creates problems for several domains in which AI or components of AI are integrated <ref type="bibr" target="#b26">[27]</ref>. For example, in the context of an IDS, CSoCs analysts are tasked with the responsibility of analyzing IDS alerts for a variety of purposes, including alert escalation, threat and attack mitigation, intelligence gathering, and forensic analysis among others <ref type="bibr" target="#b27">[28]</ref>. The lack of explanation of alerts generated by an IDS creates a barrier for task analysis and subsequently impedes decision-making.</p><p>In addition to the issues of transparency and trust surrounding AI systems, there exists yet another issue referred to as the problem of decomposability, specifically for systems built with DL models (See Section IV-A3 for IDS based on the decomposition approach). AI systems that are designed using DL techniques are difficult to interpret due to their inability to be decomposed into intuitive components <ref type="bibr" target="#b28">[29]</ref>. The difficulty in interpreting DL models jeopardizes their actual use in production, as the computation behind their decisions are unknown <ref type="bibr" target="#b10">[11]</ref>. Explainable AI (XAI) seeks to remedy this and other problems.</p><p>According to the Defense Advanced Research Projects Agency (DARPA), XAI systems are able to explain their reasoning to a human user, characterize their strengths and weaknesses, and convey a sense of their future behavior <ref type="bibr" target="#b23">[24]</ref>. In this sense, by justifying specific decisions, XAI systems aid users in comprehending the model and assisting them in maintaining and effectively using it. On the other hand, transparency about predictions contributes to the development of trust in a system's intended behavior and provides users with confidence that they are performing tasks correctly.</p><p>Transparency is an open problem in the field of intrusion detection. Cybersecurity professionals now frequently make decisions based on the recommendations of an AI-enabled IDS. Therefore, the predictions made by the model should be understandable <ref type="bibr" target="#b10">[11]</ref>. For instance, when an IDS model is presented with zero-day attacks, the model may misclassify the attacks as normal, resulting in a system breach. Understanding why specific samples are misclassified is the first step toward debugging and diagnosing the system. It is critical to provide detailed explanations for such misclassifications, so as to determine the appropriate course of action to prevent future attacks <ref type="bibr" target="#b5">[6]</ref>. Therefore, an IDS should go beyond merely detecting intrusions-i.e., it should provide reasoning for the detected threat. The explanations in the form of correlations of various factors (for example, time of intrusion, type, suspicious network flow) influencing the predicted outcome can assist cybersecurity analysts in quickly analyzing tasks and making decisions <ref type="bibr" target="#b27">[28]</ref>.</p><p>The goal of XAI in the field of intrusion detection is to build operator trust and allow for more control of autonomous AI subsystems. Explainable Intrusion Detection Systems (X-IDS) will help build trust in these systems while also aiding CSoC analysts in their task of defending systems.</p><p>The major contributions of the paper are as follows:</p><p>importantly, how these issues relate to the intrusion detection domain. We propose a taxonomy based on a literature review to help lay the groundwork for formally defining explainability in intrusion detection.</p><p>• A comprehensive survey of the current landscape of X-IDS implementations is presented, with an emphasis on two major approaches: black box and white box. The distinction between the two approaches is discussed in detail, as is the rationale for why the black box approach with post-hoc explainability is more appropriate for intrusion detection tasks.</p><p>• We propose a generic explainable architecture with a user-centric approach for designing X-IDS that can accommodate a wide variety of scenarios and applications without adhering to a specific specification or technological solution.</p><p>• We discuss the challenges inherent in designing X-IDS and make research recommendations aimed at effectively mitigating these challenges for future researchers interested in developing X-IDS. The remainder of this paper is organized as follows.</p><p>Section II provides the background on explainable artificial intelligence (XAI). Section III summarizes our survey and taxonomy. Following that, in Section IV, we review the literature on black box and white box X-IDS approaches. Section V introduces a generic X-IDS architecture that future researchers can use as a guide. Section VI identifies research challenges and makes recommendations to future researchers. Finally, Section VII concludes this survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPLAINABLE ARTIFICIAL INTELLIGENCE (XAI)</head><p>The definition of what constitutes an explanation in AI remains an open research question. In the available literature, there are various definitions of the 'explainable AI' (XAI). Lent et al. <ref type="bibr" target="#b29">[30]</ref> defines XAI as a system that ''provides an easily understandable chain of reasoning from the user's order, through the system's knowledge and inference, to the resulting behavior''. The authors of this work used the term XAI to describe their system's ability to explain the behavior of AI-controlled entities. However, the recent definition by DARPA <ref type="bibr" target="#b30">[31]</ref> indicates XAI as the intersection of different areas including machine learning, human computer interface, and end user explanation.</p><p>XAI has the potential to offer significant benefits to a broad range of domains that rely on artificial intelligence systems. Most importantly, the impact of XAI in the decision-making process for stakeholders in the IDS ecosystem is considerable. IDSs based on deep learning models can be more efficient in detecting malicious traffic with high accuracy. However, a CSoC analyst is still left with a significant task, i.e., to determine why the flow is malicious, and how to best deal with the attack. One solution to this problem is the post-hoc explainability offered by XAI. In <ref type="bibr" target="#b10">[11]</ref>, for example, the authors demonstrate how the SHapley Additive exPlanation (SHAP) framework <ref type="bibr" target="#b31">[32]</ref> can be utilized to gain a deeper understanding of the model characteristics that contribute the most to various types of attacks. Similarly, misclassification is another important issue within the IDS ecosystem. Misclassification of malicious network flow as benign could be catastrophic to an organization (CSoC). For example, the authors in <ref type="bibr" target="#b5">[6]</ref>, demonstrated how the counterfactual technique can be used to explain misclassification. User trust in IDS predictions is imperative, and providing justification is as important as the prediction itself. For example, in <ref type="bibr" target="#b32">[33]</ref>, the authors illustrated how user trust can be garnered by providing input feature relevance scores (Layer-wise Relevance Propagation (LRP) method) that indicate the contribution of each input feature to the detection of the intrusion.</p><p>Apart from IDS systems, currently, XAI is being used in mission-critical systems and defense <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. To foster the trust of AI systems in the transportation domain, researchers are proposing explanations systems <ref type="bibr" target="#b33">[34]</ref>. Some works based on image processing with explainability is found in <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, and <ref type="bibr" target="#b37">[38]</ref>. Transparency regarding decision-making processes is critical in the criminal justice system <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Various explainable methods for judicial decision support systems have been proposed by authors in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and <ref type="bibr" target="#b41">[42]</ref>. Model explainability is essential for gaining trust and acceptance of AI systems in high-stakes areas, such as healthcare, where reliability and safety are critical <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Medical anomaly detection <ref type="bibr" target="#b44">[45]</ref>, healthcare risk prediction system <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, genetics <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and healthcare image processing <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> are some of the areas that are moving towards adoption of XAI. Another area is finance, such as AI-based credit score decisions <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> and counterfeit banknotes detection <ref type="bibr" target="#b56">[57]</ref>. Support for XAI in academia for evaluation tasks are found in <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, and <ref type="bibr" target="#b59">[60]</ref>. Lastly, in the entertainment industry XAI for recommender systems is found in the works of <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, and <ref type="bibr" target="#b62">[63]</ref>.</p><p>Arrieta et al. <ref type="bibr" target="#b63">[64]</ref> argue that one of the issues that hinders the establishment of common ground for the meaning of the term 'explainability' in the context of AI is the interchangeable misuse of 'interpretability' and 'explainability' in the literature. Interpretability is the ability to explain or convey meaning in human-comprehensible terms <ref type="bibr" target="#b63">[64]</ref>. This translates into the ability of a human to understand the model's reasoning without the need for additional explanations <ref type="bibr" target="#b64">[65]</ref>. On the other hand, explainability is associated with the concept of explanation as a means of interface between humans and a decision-maker (model) that is both accurate and comprehensible to humans <ref type="bibr" target="#b64">[65]</ref>. In this sense, if system users need an explanation as a proxy system to understand the reasoning process, that explanation is precisely represented by the XAI.</p><p>A central concept that emerges from all the preceding definitions of the XAI is 'understandability', which is the degree to which a human can comprehend a decision made with respect to a model. However, understandability is tightly coupled with the characteristics of the system's users. For instance, whether or not the explanation made the concept clear or simple to understand is entirely dependent on the audience. Despite the widespread recognition of the importance of explainability, researchers are struggling to establish universal, objective criteria for developing and validating explanations <ref type="bibr" target="#b65">[66]</ref>. This is because XAI is plagued by inherent challenges that need addressing to foster its development. These include (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques <ref type="bibr" target="#b66">[67]</ref>.</p><p>To address these challenges, we propose a taxonomy as depicted in Figure <ref type="figure" target="#fig_0">1</ref> based on our literature review to lay the groundwork for formally developing and validating explanations. In the following subsections, we describe in detail the concepts related to XAI including its notions, its meaning to various stakeholders of the system, and metrics to evaluate explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NOTIONS OF EXPLAINABILITY</head><p>Several approaches to explanation methods have been proposed by different authors in the pursuit of explaining AI systems. The authors in <ref type="bibr" target="#b64">[65]</ref> conducted a survey of black box specific explainability methods and proposed a taxonomy for XAI systems based on four characteristics: (i) the nature of the problem; (ii) the type of explainer used; (iii) the type of black box model processed by the explainer; and (iv) the type of data supported by the black box.</p><p>In another work <ref type="bibr" target="#b67">[68]</ref>, the authors presented notions related to the concept of explainability in two clusters. The first cluster refers to attributes of explainability -it contains criteria and characteristics used by scholars in trying to define the construct of explainability. The second cluster refers to the theoretical approaches for structuring explanations. Das and Rad in <ref type="bibr" target="#b68">[69]</ref> proposed a taxonomy for categorizing XAI techniques based on explanation scope, algorithm methodology, and usage. Similarly, the authors in <ref type="bibr" target="#b54">[55]</ref> surveyed over 180 articles related to explainability and categorized explainability using three criteria: the complexity of interpretability, the scoop of interpretability, and model dependency. The first criterion emphasizes the difficulty of interpreting and explaining complex models, such as those based on deep learning. The second criterion differentiates between local and global explanations, while the third criterion discusses model-specific and model-agnostic explanations. On the other hand, Pantelis et al. in <ref type="bibr" target="#b69">[70]</ref> divided explainability methods into four groups based on: the data types used, the scope of explanation, the purpose of explanation, and the model usage.</p><p>A common category found in the literature regarding the taxonomy of explainability is the scope of explainability and model dependency. The following subsections describe these categories in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) LOCAL EXPLAINABILITY</head><p>The ability to explain a single prediction or decision is an example of local explainability. This explainability is used to generate a unique explanation or justification of the specific decision made by the model <ref type="bibr" target="#b54">[55]</ref>.Some of the local explanation methods include the Local Interpretable Model Agnostic Explanation (LIME) <ref type="bibr" target="#b70">[71]</ref>, the Anchors <ref type="bibr" target="#b71">[72]</ref> and the Leave One Covariate Out (LOCO) <ref type="bibr" target="#b72">[73]</ref>. LIME was originally proposed by Ribeiro et al. <ref type="bibr" target="#b70">[71]</ref> who used a surrogate model to approximate the predictions of the black box model. Rather than training a global surrogate model, LIME uses a local surrogate model to interpret individual predictions.</p><p>To explain the behavior of complex models with high precision rules called Anchors, representing local, sufficient conditions for predictions, the same authors proposed an extension to the LIME method in <ref type="bibr" target="#b71">[72]</ref>. Another popular technique for generating local explanation models with local variable importance measures is LOCO <ref type="bibr" target="#b72">[73]</ref>.</p><p>Lundberg et al. <ref type="bibr" target="#b31">[32]</ref> proposed a game-theoretic optimal solution based on Shapley values for model explainability referred to as Shapely Additive Explanations (SHAP). SHAP calculates the significance of each feature in each prediction. The authors have demonstrated the equivalence of this model among various local interpretable models including LIME <ref type="bibr" target="#b70">[71]</ref>, Deep Learning Important FeaTures (DeepLIFT) <ref type="bibr" target="#b73">[74]</ref>, and LayerWise Relevance Propagation (LRP) <ref type="bibr" target="#b74">[75]</ref>. The SHAP value can be computed for any model, not just simple linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) GLOBAL EXPLAINABILITY</head><p>The global explainability of a model makes it easier to follow the reasoning behind all the possible outcomes. These models shed light on the model's decision-making process as a whole, resulting in an understanding of the attributions for a variety of input data <ref type="bibr" target="#b68">[69]</ref>.</p><p>The LIME <ref type="bibr" target="#b70">[71]</ref> model was extended with a 'submodular pick algorithm' (SP-LIME) in order to comprehend the model's global correlations. By providing a non-redundant global decision boundary for the machine learning model, LIME provides a global understanding of the model from individual data instances using a submodular pick algorithm.</p><p>Concept Activation Vectors (CAVs) proposed by Kim et al. <ref type="bibr" target="#b75">[76]</ref> is another global explainability method. This model can interpret the internal states of a neural network in the human-friendly concept domain. In another work, Yang et al. <ref type="bibr" target="#b76">[77]</ref> proposed a novel method, the Global Interpretation via Recursive Partitioning (GIRP), to construct a global interpretation tree based on local explanations for a variety of machine learning models. Other methods of global explanation include an explanation by information extraction <ref type="bibr" target="#b77">[78]</ref>. In this study, the authors propose a method of information extraction that is only lightly supervised and provides a global interpretation. They demonstrated that interpretable models can be generated when representation learning is combined with traditional pattern-based bootstrapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) MODEL-SPECIFIC INTERPRETABILITY</head><p>The use of model-specific interpretability methods is restricted to a limited number of model classes. With these methods, we are restricted to using only models that provide a specific type of interpretation, which can reduce our options for using more accurate and representative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) MODEL-AGNOSTIC INTERPRETABILITY</head><p>Methods that are model agnostic are not tied to any specific type of ML model, and are by definition modular, in the sense that the explanatory module is unrelated to the model for which it generates explanations. Model-agnostic interpretations are used to interpret artificial neural networks (ANNs) and can be local or global. In their survey <ref type="bibr" target="#b68">[69]</ref>, the authors argue that a significant amount of research in XAI is concentrated on model-agnostic post-hoc explainability algorithms, due to their ease of integration and breadth of application. Based on other reviewed papers, the authors <ref type="bibr" target="#b54">[55]</ref> broadly categorize the techniques of model-agnostic interpretability into four types, including visualization, knowledge extraction, influence methods, and example-based explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FORMALIZING EXPLAINABILITY TASKS FROM THE USER PERSPECTIVES</head><p>To be explainable, a machine learning model must be humancomprehensible. This presents a challenge for the development of XAI because it entails communicating a complex computational process to humans. The interpretable element that serves as the foundation of explanation is highly dependent on the question of ''who'' will receive the explanation. Rosenfeld et al. <ref type="bibr" target="#b78">[79]</ref> identified three targets of explanation, including regular user, expert user, and the external entity. According to the authors, an explanation should be specific to user types. For instance, in a legal scenario, the explanation must be made to the expert users, not the regular users. On the other hand, if explanations are geared towards regular users, then the chance of developing trust and acceptance of XAI methods is high. To address the issue of stakeholder-specific explanation requirements, IBM developed an open-source toolkit known as AI Explainability 360 (AIX360) <ref type="bibr" target="#b79">[80]</ref>.</p><p>Adadi et al. <ref type="bibr" target="#b54">[55]</ref> emphasize the significance of humans-in-the-loop approach for explainable systems from two perspectives: Human-like explanation and Human-friendly explanation. The first aspect focuses on how to produce explanations that simulate the human cognitive process, while the second aspect is concerned with developing explanations that are centered on humans.</p><p>Section V discusses the importance of human-centered design when developing X-IDS systems, and Section VI-B examines the explainability requirements imposed by various stakeholders in the IDS ecosystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MEASURES FOR EVALUATING EXPLAINABILITY TECHNIQUES</head><p>There have been few studies on evaluating explanations and quantifying their relevance despite the growing body of research that produces explainable ML methods. Doshi-Velez and Kim <ref type="bibr" target="#b80">[81]</ref> proposed the three classes as evaluation methods for interpretability, including applicationgrounded, human-grounded, and functionally grounded methods. Application-grounded evaluation is concerned with the impact of the interpretation process's results on the human, domain expert, or end user, in terms of a well-defined task or application. Human-grounded evaluation is concerned with conducting simplified application-grounded evaluation where experiments are run with regular users rather than domain experts. Functionally grounded evaluation does not require human subjects, and rather uses formal, well-defined mathematical definitions of interpretability to determine the method's quality.</p><p>On the other hand, in <ref type="bibr" target="#b81">[82]</ref>, the authors outline three different evaluation criteria of explanations for deep networks, such as processing, representation, and explanation producing. The first criterion includes techniques that simulate data processing to generate insights about the relationships between a model's inputs and outputs. The second criterion describes an approach on how data is represented in networks and explains the representation. The third criterion states that the explanation-producing systems can be evaluated according to how well they match user expectations.</p><p>To evaluate local explanation, IBM in their toolkit AIX360 <ref type="bibr" target="#b79">[80]</ref> suggested two metrics such as Faithfulness <ref type="bibr" target="#b82">[83]</ref> and Monotonicity <ref type="bibr" target="#b83">[84]</ref>, to quantify the ''goodness'' of a feature-based local explanation. Other types of evaluation criteria found in the body of literature include completeness compared to the original model, ability to detect models with biases, completeness as measured on a substitute task, and human evaluation.</p><p>Another significant piece of work that could serve as a benchmark for evaluating explanations is the Florida Institute for Human and Machine Cognition's psychological model of explanation (IHMC) <ref type="bibr" target="#b23">[24]</ref>. Section VI-C provides greater detail about this proposed model.</p><p>Next, we describe our survey approach and develop a taxonomy for X-IDS grounded in the current literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SURVEY AND TAXONOMY</head><p>The term intrusion refers to any unauthorized activity occurring within a network or system. An IDS is a collec- tion of tools, methods, and resources that assist CSoC analysts in identifying, assessing, and reporting intrusions. Intrusion detection is typically a component of protection that surrounds a system and is not a stand-alone protection measure <ref type="bibr" target="#b84">[85]</ref>. IDS are classified according to where they look for intrusive behavior: host-based or networkbased. A host-based IDS monitors traffic that is originating from and coming to a specific host. Network-based IDS are strategically positioned in a network to analyze incoming and outgoing communication between network nodes.</p><p>IDS are categorized based on three detection techniques: signature-based, anomaly-based, and hybrid. Signaturebased IDS monitors network traffic and compares it to a database of known malicious threats' signatures or attributes. However, they are incapable of detecting zero-day attacks, metamorphic threats, or polymorphic threats <ref type="bibr" target="#b85">[86]</ref>. On the other hand, anomaly-based IDS look for patterns in data that do not conform to expected behavior <ref type="bibr" target="#b86">[87]</ref>, allowing them to recognize such threats. However, these detection systems are susceptible to higher false positive rates because they may categorize previously unseen, yet legitimate, system behaviors as anomalies <ref type="bibr" target="#b18">[19]</ref>. Hybrid IDS integrate both signature-based and anomaly-based detection methods, which allows for an increased detection rate of known intrusions, the ability to detect unseen intrusions, and reduce false positives.</p><p>As previously stated, prior work has focused on XAI from the lens of explainability, qualifying the definitions of notion, users, and metrics (See Section II). This survey follows that direction by creating a taxonomy surrounding current XAI techniques for IDS. The focus is on their relevance and applicability to the domain of intrusion detection, with a particular emphasis on the current hierarchy of families, strengths and weaknesses, and any challenges or assumptions that come with their application. A summary of our taxonomy can be seen in Figure <ref type="figure" target="#fig_1">2</ref>. The two primary families of XAI techniques are those of white box models and black box models which greatly affect our survey taxonomy for approaches to X-IDS. The survey of existing systems based on the taxonomy in Figure <ref type="figure" target="#fig_1">2</ref>, is available in Section IV. Next, we describe the salient features of white box models and black box models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SALIENT FEATURES OF WHITE BOX TECHNIQUES</head><p>White box models provide results that are easy to understand <ref type="bibr" target="#b87">[88]</ref>. This easy to understand condition is typically defined as an explainable outcome understood by an expert in the field. In practice, this definition is more associated with the popular suite of machine learning models that existed prior to the rise in popularity of neural network based approaches. White box models, while generally not as efficacious as their black box counterparts, bring a layer of transparency that is intrinsic to their decision process. This trait is often preferred, if not a requirement, in domains where the decision system is sensitive or requires a high degree of auditing. These models cover a wide variety of techniques that fall into four distinct families: Regression, Rule-Based, Clustering, and Statistical &amp; Probabilistic Methods.</p><p>Regression-based approaches comprise the family of regression analysis. These approaches have a well formed background of statistical support and maturity. Therefore, these models are most often employed in the early stages of modeling, in the pipelines of more complex models, and in domains where scrutiny and transparency are of paramount importance. Although not a focal point of comparison for this paper, regression models are highly computationally efficient, allowing for rapid construction, as well as deployment into low-resource systems where detection time is critical, such as IoT edge devices. Regression approaches can be split into Parametric Regression and Non-parametric Regression. The former enforces a constraint on model expectation via a restriction on the parameters of the model, making this modeling approach best for when certain assumptions can be met. The latter enforces no such constraint, which decreases overall interpretability but increases the application to a wider variety of data and assumptions. Popular regression techniques are Linear Regression (LR), Logistic Regression (LoR), various non-linear models, Poisson Regression, Kernel Regression (KR), and Spline Smoothing.</p><p>Rule-based approaches leverage a learned set of rules as a means of the model decision process, and thusly, model explainability. Rule based explanations are perhaps the most practical, as they mimic the human decision making process when it comes to defining an anomaly. This process also allows learned rules to then be incorporated into Signature-Based IDS (SIDS), allowing Anomaly-Based IDS (AIDS) to serve as zero-day identifiers and rule miners. Rulebased approaches benefit from the allowance of a very tight definition of rules, known as hard rules or crisp rules, or for a relaxed fuzzy-rule based approach, allowing flexibility and further statistical inference to be rendered on them. A popular approach to modeling for rule-based explanations is the Decision Tree and its many variants.</p><p>Statistical &amp; Probabilistic Methods is a broad category for the numerous statistical models of reasoning that exist in the literature. Notably, many of these methods have seen a decline in use as a compliment to the rise in popularity of various black box methods. These less frequently used methods are appropriate for application in specific scenarios or in larger pipelines for multi-stage IDS. Examples of such approaches include moment-based approaches, statistical ensembles, Markov Models, Baysian Networks, and others which are covered more specifically in IV-B3.</p><p>Clustering-based approaches use supervised or unsupervised learning to aggregate similar data objects. This similar condition is defined by a similarity, or dissimilarity, measure. Traditionally these methods are defined on distance based metrics such as Euclidean, Manhattan, Cosine Measure, Pearson coefficient, and many others. Other attempts to define similarity have had success in the graph representation domain, using graph-based clustering algorithms to accomplish this task. Clustering, due to its ability to be leveraged as an unsupervised learner, still retains a high degree of use due to the importance of data mining for intrusion detection. Examples of popular clustering algorithms are K-Means, Self-Organzing Maps (SOMs), Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Agglomerative Clustering, and Spectral Clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SALIENT FEATURES OF BLACK BOX TECHNIQUES</head><p>Black box models are models where the decision systems are considered opaque <ref type="bibr" target="#b64">[65]</ref>. These systems, composing nearly all of the state-of-the-art, are limited due to the lacking ability of model inspection and evaluation. Therefore, if these systems are to be utilized in decision sensitive domains, i.e, those whose applications require safety, privacy, and fairness, some degree of exploration and evaluation of their decision process must be possible. Currently, there exists no singular solution to the black box inspection problem. However, many candidate explanations have emerged, exploring and exploiting various aspects of the machine learning process to create explanations for black box models. These candidate explanations currently fall into four distinct families: Feature, Perturbation, Decomposition, and Hybrid.</p><p>Feature-based explanations target features as the method of explanation. The goal of feature attribution is to determine how much each feature is responsible for the output prediction. Features were one of the first methods of explainability in black box models due to their impact on model performance and human interpretable relevance. Examples of popular feature based explanations are Partial Dependence Plot (PDP) <ref type="bibr" target="#b88">[89]</ref>, Accumulated Local Effects (ALE) <ref type="bibr" target="#b89">[90]</ref>, Individual Conditional Expectations (ICE) <ref type="bibr" target="#b90">[91]</ref>, H-statistic <ref type="bibr" target="#b91">[92]</ref>, and SHapley Additive exPlanations (SHAP) <ref type="bibr" target="#b31">[32]</ref>.</p><p>Perturbation-based explanations study changes to the output space with perturbations to the input space. Due to this property, perturbation techniques can be deployed to any general input space, such as tabular data, images, or text. In particular, model sensitivity to feature perturbations has long been regarded as a measure of feature importance. Saliency maps, Randomized Input Sampling for Explanation of Black Box Models (RISE) <ref type="bibr" target="#b92">[93]</ref>, and Local Interpretable Model-Agnostic Explanations (LIME) <ref type="bibr" target="#b70">[71]</ref> are popular perturbation methods.</p><p>Decomposition-based explanations decompose the original model prediction. Much like the previous two methods, the goal of decomposition is to allocate a measure of importance to the input space; however, this method does so by the decomposition of a model signal, such as the model's gradients. This is predicated on the assumption that large gradients play a role in shaping explanations. However, gradients are not the only method of decomposition. Many approaches exist, such as Gradient * Input <ref type="bibr" target="#b93">[94]</ref>, Integrated Gradients (IG) <ref type="bibr" target="#b94">[95]</ref>, Grad-CAM <ref type="bibr" target="#b95">[96]</ref>, DeepLIFT <ref type="bibr" target="#b73">[74]</ref>, Deep Taylor Decomposition (DTD) <ref type="bibr" target="#b96">[97]</ref> and Layerwise Relevance Propagation (LRP) <ref type="bibr" target="#b74">[75]</ref>.</p><p>Hybrid-based explanations encapsulate a type of model construction often demonstrated in pipelined machine learning architectures. These models can range from ensembles, a blend of white box and black box approaches working in tandem, to carefully composed IDS pipelines encapsulating many of the best state-of-the-art approaches. Therefore, hybrid approaches present the most variability of explanations, with respect to methodology, location of explanations, and application.</p><p>Next, we use the taxonomy showcased in Figure <ref type="figure" target="#fig_1">2</ref>, to present a literature survey on approaches to X-IDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACHES TO EXPLAINABLE IDS (X-IDS)</head><p>As per the survey overview presented in Section III, and the taxonomy showcased in Figure <ref type="figure" target="#fig_1">2</ref>, we will now describe in detail the black box and the white box approaches to XAI in intrusion detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BLACK BOX X-IDS MODELS</head><p>Guidotti et al. <ref type="bibr" target="#b64">[65]</ref>, describes a black box predictor as ''a data-mining and machine learning obscure model, whose internals are either unknown to the observer or are known but are uninterpretable by humans''. A black box model is not explainable by itself. Therefore, to make a black box model explainable, we have to adopt several techniques to extract explanations from the inner logic or the outputs of the model.</p><p>To survey the IDS landscape with respect to explainability, we have further divided the literature into different categories of XAI black box models: feature based, perturbations based, decomposition based, and hybrid approaches. These classifications are based upon how explanations are generated. A detailed literature overview is also available in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) FEATURE BASED APPROACHES</head><p>One popular scheme for explanations considers the influence features have on prediction. Such schemes are called feature explanations. Existing processes, such as feature engineering and feature selection, are already common in machine learning pipelines. Therefore, it is natural that features emerge as a method of explainability. Several candidate solutions that currently exploit this assumption are Partial Dependence Plot (PDP), Accumulated Local Effects (ALE), H-statistic, and SHAP.</p><p>An important generalizable SHAP-based framework is proposed by Wang et al. <ref type="bibr" target="#b10">[11]</ref>. Their framework uses both local and global explanations to increase the explainability of the IDS model. The IDS model consists of a binary Neural Network (NN) classifier and a multi-class NN classifier. To generate explanations, both models and predictions are fed to the SHAP module. Local explanations are generated by choosing an attack and randomly selecting 100 of the occurrences. An average Shapely value is calculated, and the SHAP module outputs a confidence score for the prediction. The authors evaluate explainability by using a neptune attack, where a flooding of SYN packets is observed. The explanation results show that the top four features are related to DoS and SYN flood attacks. Using the global explanation produced by the SHAP module, researchers can make inferences about how the model might react during a related attack. However, the model's confidence seems to favor attacks that attempt many network connections (e.g. probe or DoS) over other attacks, such as privilege escalation attacks. The IDS system along with the SHAP explanations are relevant to assist subject matter experts in making security decisions.</p><p>In another effort, Islam et al. <ref type="bibr" target="#b97">[98]</ref> built a domain knowledge infused explainable IDS framework. Their architecture is composed of two parts: a feature generalizer that uses the CIA principles and an evaluator that compares the black box models using different configurations.</p><p>The feature generalizer first maps the top three ranked features to attack types, then maps attack types to the CIA principles. For example, DoS attacks are associated with availability; Heartbleed or PortScan attacks are associated with confidentiality. Using this mapping system, the authors add three new features: C, I, and A. These three features include the aggregate scores of their related features from a data sample. If a feature positively affects a prediction, then it adds to the score; otherwise, it subtracts from the score.</p><p>The evaluator, on the other hand, runs four different feature configurations. The first configuration uses the full, preprocessed CICIDS dataset of 78 features. The second is a feature selected dataset of 50 attributes. The final two datasets are domain knowledge based: a 22 feature dataset of domain infused features and a three feature dataset consisting of C, I, and A scoring features.</p><p>Tests are run on ANN, SVM, Random Forest (RF), Extra Trees (ET), Gradient Boosting (GB), and Naive Bayes algorithms (NB). The authors outline two types of tests: explainability and generalizability. The first two datasets are used to find a baseline to compare against the authors' novel, domain infused approach. Their findings from initial experimentation show that the RF using the full dataset outperforms all other algorithms with an F1-score of 99.68%. The domain infused and CIA datasets are able to obtain an F1-score of 99.32% and 93.84% on RF and ET algorithms, respectfully. The small difference between the full dataset and the domain infused dataset show that the authors can now create a way to explain predictions without negatively impacting model performance. The authors create another CIA scoring formula that shows how much impact a CIA mapped feature had on the samples prediction. These C, I, and A scores can then be shown to an analyst to explain the prediction. To test their method against unknown attacks, the models are trained on all attacks in the dataset except one. The classifier is tested on a dataset that includes all of the attacks.</p><p>The results show that the novel domain infused dataset performs similarly to the full dataset. In one case, the domain infused dataset is able to be used to find an attack that the full dataset configuration could not. The authors have demonstrated that creating an explainable algorithm and dataset can be useful for both accuracy and explanations.</p><p>Sarhan et al. proposes another feature based technique <ref type="bibr" target="#b98">[99]</ref>. Two feature sets, NetFlow and CICFlowMeter, are evaluated across three datasets. When new IDS datasets are created, they are not necessarily created using the same tools. NetFlow and CICFlowMeter based IDS datasets collect different feature sets. The authors test these different feature sets using Random Forests and Deep Feed Forward algorithms. The results from this experiment show a minor improvement from the NetFlow feature set over the CICFlowMeter set. The most interesting result is the change in false positives between the two feature sets. NetFlow offers a much lower false positive rate than its counterpart in many of the tests. Additionally, NetFlow is slightly faster to make predictions than CICFlowMeter. The authors conclude that NetFlow offers slightly higher quality security features. Explainability is achieved in the form of SHAP. SHAP is used to determine which features are causing this difference in performance. The authors conclude that there are certain features across all datasets that contain more security focused data. However, the most important features vary across datasets. This is attributed to the fact that each dataset has different attacks. The authors work shows the importance of feature selection during dataset creation.</p><p>A novel method in <ref type="bibr" target="#b99">[100]</ref> uses Auto-Encoders (AE) in combination with SHAP to explain anomalies. Anomalies TABLE 1. An overview of the existing literature on black-box approaches to intrusion detection systems, with a focus on their scope, contribution, and limitations. TABLE 1. (Continued.) An overview of the existing literature on black-box approaches to intrusion detection systems, with a focus on their scope, contribution, and limitations.</p><p>are detected using the reconstruction score of the AE. Samples that return a higher reconstruction score are considered anomalous. An explainer module is created with the goal to link the input value of anomalies to their high reconstruction score. Features are split into two sets. The first set contains features that are causing the reconstruction score to be higher, while the second set does the opposite. The authors label these sets 'contributing' and 'offsetting', respectively. Contributing features will have a SHAP score that is negative, and the opposite is true for offsetting. Explanations are presented in the form of a color-coded table where darker values are more important than lighter values. This novel approach to explaining AE can be improved with more iterations of its visualization style and methodology.</p><p>In another piece of work, Dang, Q. V. <ref type="bibr" target="#b107">[108]</ref> suggests an explainable IDS that uses the eXtreme Gradient Boosting (xgboost) classifier as its base predictor model and makes explanations using PDP plots and the SHAP value. The author uses the CICIDS2017 dataset to train and test the proposed model. The experiment result indicates the proposed classifier has high detection accuracy. However, it requires high computational power. To reduce the computational need, the author utilizes the PDP plots to recursively remove features that cannot be explained without affecting their predictive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) PERTURBATION BASED APPROACHES</head><p>Perturbation based approaches make minor modifications to input data to observe changes in output predictions. Their explanations are based on the inclusion, removal, or modification of a feature in a dataset. These approaches are model agnostic (see Section II), therefore, they can be applied to any model.</p><p>A representative work by Wu et al. <ref type="bibr" target="#b101">[102]</ref> showcases the advantages of this approach. The authors have created a CNN model along with a dashboard user interface (UI) to make the black box deep learning components more explainable. They gather feature requirements for their dashboard from literature. These include: (i) it is important to know the role that individual neurons play in predictions; (ii) multiple models should be tested, and the best parameters should be selected to achieve the best accuracy; (iii) visualization should assist in finding interesting results; (iv) there should be an explanation as to how the model made a decision; (v) we should be able to see the data representation in each layer of the model.</p><p>The authors use the NSL-KDD dataset to test their CNN. NSL-KDD is encoded into a 12 × 12 grayscale image that serves as input. Their model is able to achieve an 80% accuracy. The dashboard UI is able to showcase a variety of visualizations that assists in explainability. The UI includes: a detailed view of each cluster of neurons and the associated feature class, a t-SNE scatterplot of the activation values, a feature map of the convolutional kernel, a feature panel that explains how the model came to a prediction (utilizing LIME and a Saliency chart), a confusion matrix of predicted instances, and a graph for finding input data patterns. The authors demonstrate the advantages of using the dashboard UI by comparing CNNs with fewer layers than their proposed architecture. For example, the last layer in a smaller CNN shows that it is unable to detect one of the attack types (u2r) from the NSL-KDD dataset, while the proposed architecture can detect the attack. The dashboard UI is able to demonstrate that the smaller model may need more layers to be effective.</p><p>Khan et al. <ref type="bibr" target="#b100">[101]</ref> propose an explainable autoencoder-based detection framework using convolutional and recurrent networks to discover cyber threats in IoT networks. The model is capable of detecting both known and zero-day attacks. It leverages a 2-step, sliding window technique that is used to transform a 1-dimensional (1D) sample into smaller contiguous 2-dimensional (2D) samples. This 2D sample is then fed through a CNN, comprised of a 1D convolutional layer and a 1D max-pooling layer which extracts spatial features. The data is then fed into the auto-encoder based LSTM that extracts temporal features. Finally, the DNN uses the extracted representation to make predictions. To make the model explainable, the authors use LIME <ref type="bibr" target="#b70">[71]</ref> (see Section II-A). The dataset used for experimentation was from a real-world gas pipeline system. It consists of system logs that include packet data used to communicate with the pipeline, along with features such as packet length, pressure setpoint, and PID gain. The authors obtain a 99.35% accuracy using their proposed model. LIME shows that there are five features in the dataset that are primarily responsible for the different predictions.</p><p>In another impactful work <ref type="bibr" target="#b5">[6]</ref>, the authors argue that rather than explaining every prediction, it is possible to create a model that explains misclassifications using a counterfactual technique. The goal is to explain adversarial attacks, which aim to confuse models into misclassifying input samples. Using this technique, the authors find weak points in their model and develop strategies to overcome these limitations. When an input sample is classified incorrectly, minimal changes are made to the sample until it is classified correctly. The difference between the original, incorrectly labeled sample and the new, correctly labeled sample are used to explain the occurrence of the misclassification.</p><p>NSL-KDD dataset is used to create these models. A linear classifier and a multi-layer perceptron (MLP) are used during testing and the authors achieve an accuracy of 93% and 95%, respectfully. t-SNE is used to visualize the misclassified and corrected samples. The authors technique for minimizing the difference between samples is effective as the projections created by t-SNE are nearly identical. More insight can then be gathered from these projections as they show which features caused the misclassification along with the magnitude of the impact. This method appears to be a good way to communicate why a classification occurred and allows for a user to make the necessary inferences.</p><p>Burkart et al. <ref type="bibr" target="#b102">[103]</ref> proposes a similar application of counterfactuals on an explainable IDS framework. Here the goal of the system is to answer the question: Why did X happen and not Y? The authors aim to create explanations that are understandable and actionable. By understandable, they mean explaining an instance of classification, and by actionable they mean giving advice for changing the classification. The framework should also allow the users to simulate these changes themselves. The counterfactual technique is used to achieve these goals.</p><p>The technique takes a vector x and locates a similar co-ordinate position x in the feature space, that causes a change in the predicted label. x should be a sample that is very similar to x. The authors' method for their explainable technique has 5 phases. In Phase 1, their algorithm finds the first counterfactual point by using an optimization problem. Phase 2 extrapolates that point by finding other points near it that are also opposite of the original vector x. By adding more than one counterfactual point, the algorithm can help find a better general understanding of the feature area. In their approach, the authors use MagneticSampling to achieve this goal. This set of points is used in Phase 3 to find the decision Relevance scores (R j , R k ) are calculated backwards from the output for each layer (j and k represent neurons). Scores from each previous layer are used to score the next set of neurons with the final outcome being the importance of each input <ref type="bibr" target="#b109">[110]</ref>.</p><p>boundary. Phase 4 takes this approximated decision boundary and trains a surrogate explainer model for samples on both sides of the decision boundary. Phase 5 is the culmination of all of the previous work resulting in explanations, which include a feature importance explanation, a relative difference explanation, and a surrogate visualization module. The surrogate visualization can be done in a variety of ways; however, the authors choose to use a white box decision tree. The fidelity of their explainer is tested against LIME. Additionally, their method tests 2 varieties of explainers: a decision tree explainer and a linear explainer similar to LIME. The authors method performs better than LIME when MagneticSampling is used in Phase 2, but performs worse than LIME when random sampling is used. The tree performs better than the linear method and the authors believe that it is superior based on its performance and inbuilt explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) DECOMPOSITION BASED APPROACHES</head><p>Decomposition based approaches decompose the output of a model to create a relevance score. Layer-wise Relevance Propagation (LRP) is a technique where the scoring mechanism propagates backwards from the output node, highlighting activated neurons that impact predictions. According to the authors in <ref type="bibr" target="#b108">[109]</ref>, these approaches can either decompose the output or decompose the gradient of the model.</p><p>An explainable DNN using LRP has been proposed in <ref type="bibr" target="#b103">[104]</ref>. The goal of this system is to give a confidence score of a prediction, give a textual explanation of a prediction, and the reasons why the prediction was chosen. Online, a user can see that an anomaly has been found and why it is considered an anomaly, while offline an expert can evaluate the explanations. The authors argue that the explanation for detected anomalies is provided to reduce the 'opaqueness' of DNN model and enhance 'human trust' in the algorithm. For their experiment, the authors create a partial implementation of their framework consisting of a Feed Forward DNN with explanations created by LRP. NSL-KDD is used for their experimentation. The tests are run using 4 different DNN configuration: two with three hidden layers and two with four hidden layers. Additionally, the dataset was separated into a 'simple' dataset (a smaller number of features) and a 'complete' dataset (all the features). The authors were able to achieve up to 97% accuracy from each of the implementations. The model performed better with the complete dataset rather than with the simple dataset. The authors argue that the explainability of the simple dataset is worse than the complete dataset. This is because LRP chose a feature that would be difficult for a domain expert to verify. For example, binary features like 'flag' are more difficult to explain than continuous features. The most important features for the complete dataset contained continuous values that could more easily be determined to be anomalous (src byte count and destination host count). Although the authors do not create a complete implementation with full textual explanations, their methodology could prove useful to improving the trust of regular users.</p><p>To address the issue of decomposability of DL models, the authors in <ref type="bibr" target="#b110">[111]</ref> propose an IDS system, based on CNNs, called GRACE (GRad-CAM-enhAnced Convolution neural nEtwork). They generate visual explanations for CNN decisions by utilizing the Gradient-weighted Class Activation Mapping (Grad-CAM) <ref type="bibr" target="#b95">[96]</ref>.</p><p>The authors use three different datasets including KDD-CUP-99, NSL-KDDCUP99, and UNSW-NB15 to train their model. The textual dataset is transformed using image encoding which converts the training sample from the 1D feature vector form X 1D with size 1 ×M to the 2D image form X 2D with size m ×m (with M ≤ m 2 ) and fed to the CNN model. The final convolution layer of 2D CNN is used to create heatmaps of class activations on input images, i.e., 2D grids of scores. Each pixel in the grid represents traffic characteristics, e.g., Destination port (X1) or idle max(X77). The scoring mechanism demonstrates how important each pixel (feature) is to a specific output class. This understanding of the most important features aids in the feature engineering process, resulting in a CNN model with higher accuracy.</p><p>To evaluate the performance of the model three evaluation metrics are used such as F1-Score (F1), Accuracy (A), and Computational complexity (T) (time spent to train the model). Of these, F1 and A metrics are used to compare against state-of-the-art such as CNN, GAN, LSTM, RNN, Triplet, DNN, MLP, and Autoencoder. Experimental results suggest GRACE generally outperforms its competitors. However, there are a few exceptions where the proposed model suffers slightly. For instance, when using the NSL-KDD dataset, the Triplet methods obtain 86.6% and 87.0% of A and F1, respectively, compared against 85.7% and 86.8% of the proposed model. The authors argue that this explanation approach can aid in the development of a more robust intrusion detection model.</p><p>Kauffmann et al. <ref type="bibr" target="#b111">[112]</ref> propose another decomposition strategy aimed at verifying that a 'Clever Hans' strategy has not been adopted by the ML model. LRP is leveraged as an explainer module to aid in discovering this phenomenon. Three separate models are trained: a kernel density estimator, an autoencoder, and a deep one-class model. Image based anomaly detection datasets MNIST-C and MVTec are used for this experiment. A 'Clever Hans' score is adopted that is simply the difference between the detection accuracy and explanation accuracy. Detection accuracy is the ROC score, and explanation accuracy is the cosine similarity between the ground-truth and the pixel-wise explanation. It renders a score between 1 and -1 where 1 expresses a 'Clever Hans' phenomenon. Results from their testing show that, based on their scoring system, all of the models show some form of 'Clever Hans' logic. To address this problem, the authors propose a method of bagging anomaly detectors. This solution does not remove the phenomenon, but it does help to reduce it.</p><p>The previous authors also explore Deep Taylor Decomposition (DTD) for model explainability <ref type="bibr" target="#b104">[105]</ref>. DTD is a technique that decomposes each neuron in each layer to determine feature relevance. A 'neuralized', one-class SVM is proposed that can be explained using DTD. The 'neuralized' form is a mapping of distance between the original sample and the SVM created support vectors as the first layer. The second layer is a soft min-pooling layer that calculates the 'outlierness' of samples. Samples can then be explained using DTD by decomposing each of the neurons in the prediction. In their experiment, they use image based datasets for finding anomalies. DTD is used to highlight anomalous pixels in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) HYBRID APPROACHES</head><p>A hybrid black box predictor, white box explainer has been created by Szczepanski et al. <ref type="bibr" target="#b105">[106]</ref>. Their framework is built with principles from the ''XAI Desiderata'': Fidelity, Understandability, Sufficiency, Low Construction Overhead, and Efficiency <ref type="bibr" target="#b112">[113]</ref>. The authors aim to contribute a system that is reliable, easy to understand, flexible, and meets all previous criteria without losing accuracy. With these goals in mind, a framework that uses local explanations is created. Their framework includes an ANN that predicts samples and a white box explainer that takes the output of the ANN and the original sample as input. The explainer is model agnostic and replaceable with any other explanation algorithm. The authors' explainer uses a clustering algorithm that uses a heuristic called Mean Distance to Average Vector. Clustering is done based on all of the attributes except the label. n centroids are computed for all features, then a model is trained for each centroid cluster created. Another distance based algorithm is used to find a centroid cluster that is both close to the predicted sample and gives the same prediction as the ANN. The selected cluster is then used as a visualized explanation for a prediction. The authors note that it is possible that the explainer may not return a valid tree and that the model should be trained on a feature rich, diverse dataset. The authors experiment using the CICIDS2017 dataset. The ANN is able to achieve an accuracy of 98%, and the explainer is able to achieve an accuracy of 99% with 200 clusters. The authors have created a system where there are effectively two predictors that are used to confirm and explain the other's prediction.</p><p>Pang et al. <ref type="bibr" target="#b106">[107]</ref> create a framework based on Few-Shot Anomaly Detection (FSAD). The authors claim that their framework is interpretable and explainable through a prob-ability based scoring method and an image demonstrating anomalous areas found in samples. One of the problems faced in IDS/Anomaly Detection is that models are generally trained on unsupervised, normal data. This makes it difficult for models to discern from normal and anomalous data. The authors aim using FSAD to improve detection rates. However, FSAD has difficulties learning a generalized representation of anomalies from a few samples and it is challenging to learn a robust representation of data with respect to anomolous data. To resolve this, the framework needs to be able to learn about anomalous samples but not learn that all anomalies are the same as the training samples. The authors achieve this by using a prior driven anomaly score and end-to-end optimization of anomaly scores with deviation learning based on the prior probability. The architecture of DevNet is composed of an Anomaly Scoring Network and a Reference Score Generator that outputs into a Multiple-Instance-Learning-based (MIL) deviation loss Score Learner. The Anomaly Scoring Network is a function φ that creates a scalar anomaly score for pieces of an input. In this case, the pieces of an input are parts of an image. The Reference Score Generator creates a reference score µ r , which is a mean score of randomly selected non-anomalous samples. The reference score is derived from a prior F. The function φ(X ), µ r , and the standard deviation of µ r are provided as input into the MIL Deviation Loss Learner whereby the goal is to optimize anomaly scores so that anomalies deviate significantly from normal samples.</p><p>The framework is tested on a variety of image datasets for identifying defects, planetary bodies, and medical anomalies. DevNet is tested against five other models and performs better on 7 out of 9 datasets. DevNet is able to achieve an AUC score between 80% to 98% amongst all of the datasets. As for explainability, the authors demonstrate that the algorithm can display the anomalous region on an image. DevNet generates both a black-white image of the location of the defect and an overlaid image showing where the defect lies on the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WHITE BOX X-IDS MODELS</head><p>Models that can provide an explanation to expert users without utilizing additional models are referred to as interpretable or white box models <ref type="bibr" target="#b87">[88]</ref>. A white box model's internal logic and programming steps are completely transparent, resulting in an interpretable decision process <ref type="bibr" target="#b113">[114]</ref>. However, when the model is to be explained to non-expert users, it may demand post-hoc explainability, such as visualizations <ref type="bibr" target="#b63">[64]</ref>. This interpretability, on the other hand, usually comes at a price in terms of performance <ref type="bibr" target="#b114">[115]</ref>.</p><p>A myriad of white box approaches are available for intrusion detection. Our survey will focus on the approaches most commonly used in the literature, as per our overview presented in Section III and the taxonomy showcased in Figure <ref type="figure" target="#fig_1">2</ref>. Table <ref type="table">2</ref> summarizes state-of-the-art research, challenges, and contributions with respect to white box approaches for intrusion detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) REGRESSION</head><p>Linear Regression (LR) is a supervised ML technique that establishes a relationship between a dependent variable and independent variables by computing a best fit line. The linearity of the learned relationship puts LR under the umbrella of interpretable models.</p><p>Various regression-based IDS models exist in the literature. Subba et al. <ref type="bibr" target="#b121">[122]</ref> deployed anomaly-based intrusion detection systems using two different statistical methods: Linear Discriminant Analysis (LDA) and LoR. While LR models are desirable for intrusion detection purposes, their performance is susceptible to outliers <ref type="bibr" target="#b122">[123]</ref>. To mitigate the impact of outliers, the same authors proposed a robust regression method for anomaly detection <ref type="bibr" target="#b123">[124]</ref>. The proposed method uses heteroscedasticity and a huber loss function instead of homoscedasticity and sum of squared errors.</p><p>While the existing approaches render promising outcomes, none of them were designed with explainability in mind. To overcome the issue of explainability in the area of hardware performance counter (HPC) -based intrusion detection, Kuruvila et al. <ref type="bibr" target="#b115">[116]</ref> propose an explainable HPC-based Double Regression (HPCDR) ML framework. The study examines two distinct types of attacks: microarchitectural and malware. For the first type of attack, tests are conducted on five distinct datasets: Rowhammer, Flush+Flush, Spectre, Meltdown, and ZombieLoad. For the second attack, two distinct datasets are considered: Bashlite and PNScan. To minimize computational overhead, the proposed study employs Ridge Regression (RR) rather than Shapely values to generate interpretable results. First, the three ML models (RF, DT, and NN) are chosen to evaluate the classification accuracy. Second, the output from these models is perturbed and passed to the first RR model where HPCs are employed as features and weight coefficients are received. These furnished coefficients are run on the second RR model, which identifies the most malicious sample. The authors argue that by utilizing double regression techniques, their proposed method provides transparency, which enables users to locate malicious instructions within the program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) DECISION TREE AND RULE BASED</head><p>A Decision Tree (DT) is a tree structure with decision support system elements based on graph theory. In contrast to LR method, it works even when the relationship between input and output is nonlinear. In their simplest form, DT possesses three properties that make them interpretable <ref type="bibr" target="#b28">[29]</ref>: simulatability, decomposability, and algorithmic transparency.</p><p>A simple rule is typically represented as a logical implication of IF-THEN statements by combining relational statements to form their knowledge <ref type="bibr" target="#b124">[125]</ref>. These rules can be extracted from DT. Rule-based models are considered transparent because they generate rules to explain their predictions.</p><p>Mahbooba et al. <ref type="bibr" target="#b116">[117]</ref> approach the task of developing an interpretable model to identify malicious nodes for IDS using a DT on the KDD dataset. They chose the Iterative TABLE 2. A summary of the existing literature on white-box approaches to intrusion detection systems, with an emphasis on their scope, contribution, and limitations.</p><p>Dichotomiser 3 (ID3) algorithm to ensure interpretability because it mimics a human-based decision strategy. The authors demonstrate that the algorithm can rank the relevance of features, provide explainable rules, and reach a level of accuracy comparable to state-of-the-art. Another explainable decision tree model is proposed in <ref type="bibr" target="#b124">[125]</ref> and <ref type="bibr" target="#b125">[126]</ref>, with the latter being an extension of work in <ref type="bibr" target="#b126">[127]</ref>.</p><p>Sinclair et al. <ref type="bibr" target="#b127">[128]</ref> extract rules using a DT and a Genetic Algorithm (GA) for improving the performance of the IDS model. The authors in <ref type="bibr" target="#b128">[129]</ref> and <ref type="bibr" target="#b129">[130]</ref> focus on optimizing the IDS model by extracting rules using a GA. To add transparency to the decision process, Dias et al. <ref type="bibr" target="#b117">[118]</ref> proposed an interpretable and explainable hybrid intrusion detection system. The proposed system integrates expert-written rules and dynamic knowledge generated by a DT algorithm. The authors suggest that the model can achieve explainability through the justifications of each diagnosis. Justification of certain predictions is provided in a tree-like format in the form of a suggested rule that provides a more intuitive and straightforward understanding of the diagnosis.</p><p>Snort is the world's most widely used open-source rulebased intrusion prevention system (IPS) <ref type="bibr" target="#b130">[131]</ref>. It employs a set of rules that help define malicious network activity. These rules are then used to identify packets and generate alerts for users <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) STATISTICAL AND PROBABILISTIC METHODS</head><p>In statistics, the mean, standard deviation, and any other type of correlation are referred to as moments <ref type="bibr" target="#b132">[133]</ref>. Statistical and probabilistic methods use this information to determine whether the given event is anomalous or not. The moment is predicted anomalous if they are either above or below a predefined interval. This approach is further divided into the univariate, multivariate, time series, parametric, nonparametric, operational and Markov models <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>.</p><p>Various IDS based on statistical and probabilistic models have been proposed. IDS based on the mean and standard deviation is explained in <ref type="bibr" target="#b136">[137]</ref>, while a study relating to multivariate modeling is proposed in <ref type="bibr" target="#b137">[138]</ref>. Gyanchandani et al. <ref type="bibr" target="#b138">[139]</ref> proposed an IDS based on the Markov process.</p><p>A different approach to intrinsically explainable statistical methods for network intrusion detection is proposed by Pontes et al. <ref type="bibr" target="#b118">[119]</ref>. They introduce a novel Energy-based Flow Classifier (EFC) that utilizes inverse Potts models to infer anomaly scores based on labeled benign examples. This method is capable of accurately performing binary flow classification on DDoS attacks. They perform experiments on three different datasets: CIDDS-001, CIC-IDS2017, and CICDDoS19. Results indicate that the proposed model is more adaptable to different data distributions than classical ML-based classifiers. Additionally, they argue that their model is naturally interpretable and that individual parameter values can be analyzed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) CLUSTERING</head><p>Clustering is the most widely used strategy for unsupervised ML. It classifies samples according to a similarity criterion. Clustering algorithms that can be explained have several advantages. The primary benefit of explainable clustering is that it summarizes the input behavior patterns within clusters, enabling users to comprehend the clusters' underlying commonalities <ref type="bibr" target="#b119">[120]</ref>. As stated in Section III-A there are various clustering algorithms available. However, in the context of X-IDS, we will only focus on Self-Organizing Maps (SOMs).</p><p>SOMs are an unsupervised clustering technique within the artificial neural networks umbrella. It has two layers: an input layer that accepts high dimensional space and an output layer that generates a non-linear mapping of highdimensional space into reduced dimensions. It is trained to produce a low dimensional representation of a large training dataset while preserving important topological and metric relationships of the input data <ref type="bibr" target="#b139">[140]</ref>.</p><p>An anomaly detection system using SOM techniques based on offline audit trail data is proposed in <ref type="bibr" target="#b140">[141]</ref>. The major shortcoming of the proposed system is it does not allow for real-time detection. On the other hand, the authors in <ref type="bibr" target="#b141">[142]</ref> propose Hierarchical SOMs (HSOM) for host-based intrusion detection on computer networks that are capable of operating on real-time data without requiring extensive offline training or expert knowledge. Another model based on HSOM is proposed in <ref type="bibr" target="#b142">[143]</ref>. Wickramasinghe et al. <ref type="bibr" target="#b119">[120]</ref> developed a novel model-specific explainable technique for the SOM algorithm that generates both local and global explanations for Cyber-Physical Systems (CPS) security. They used the SOMs training approach (winner-take-all algorithm) together with visual data mining capabilities (Histograms, t-SNE, Heat Maps, and U-Matrix) of SOMs to make the algorithm explainable.</p><p>A 3D color hexagonal SOM for visual intrusion detection called ANNaBell Island is proposed in <ref type="bibr" target="#b120">[121]</ref> which is an extension of 1D ANNaBell reported in <ref type="bibr" target="#b143">[144]</ref> and <ref type="bibr" target="#b144">[145]</ref>. To make the SOM process and its output explainable to the users, the authors designed a hexagonal SOM in a metahexagonal layout, referred to as an island, that graphically displayed features of network traffic. The output of the SOM model was used to create a color-separated 3D landscaped island that represents various types of network traffic, distinguishing between malicious and normal behavior.</p><p>After surveying the current black box and white box approaches to X-IDS, we propose in the next section, a generic explainable architecture with a user-centric approach for designing X-IDS that can accommodate a wide variety of scenarios and applications without adhering to a specification or technological solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DESIGNING AN EXPLAINABLE IDS (X-IDS)</head><p>The purpose of an IDS is to continuously monitor a network for malicious activity or security violations known as incidents of intrusion. If found, intrusions are reported to the cybersecurity professional responsible for monitoring such systems. A significant problem with AI based IDS is their high false positive and false negative rates. Recently, many IDS based on ML/DL techniques have been proposed to address this issue, such as DNN <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b145">[146]</ref>, RNN <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b147">[148]</ref>, and CNN <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b149">[150]</ref>. These techniques yield unprecedented detection accuracy. However, the effective use of these approaches require using highquality data, as well as a considerable amount of computing resources <ref type="bibr" target="#b150">[151]</ref>. Additionally, this modeling approach has FIGURE 4. Recommended architecture for the design of an X-IDS based on DARPA <ref type="bibr" target="#b23">[24]</ref>. The layered architecture is divided into three phases: pre-modeling, modeling, and post-modeling explainability. Each phase contributes to the development of an explanation for various stakeholders, thereby assisting in decision-making.</p><p>typically suffered from model bias, a lack of decision process transparency, and a lack of user trust.</p><p>The IDS systems based on ML/DL techniques are designed to generate event logs in the form of 'benign' or 'malicious' classification reports, that can be further analyzed by CSoC analysts. However, they do not showcase the connection between the inputs and output (i.e., they fail to indicate the reasoning behind the decision). To be more precise, a cybersecurity specialist serves as a user who reviews IDS results, but is not a component of the intrusion detection process <ref type="bibr" target="#b151">[152]</ref>. In turn, this creates a larger problem for CSoC experts, as they are unable to optimize their decisions based on the model's decision process.</p><p>To address this semantic gap, one promising technique is to design X-IDS with a human-in-the-loop approach. Typically, methods that are retraceable, explainable, and supported by visualizations amplify cybersecurity analysts' understanding in managing cybersecurity incidents in both proactive and reactive manners.</p><p>In the following sub-sections, we explain the recommended architecture, as depicted in Figure <ref type="figure">4</ref>, that could be used as guidance to design an Explainable Intrusion Detection Systems (X-IDS). The X-IDS architecture proposed in this paper is based on the DARPA recommended architecture for the design of XAI systems <ref type="bibr" target="#b23">[24]</ref>. The layered architecture consists of three phases: pre-modeling phase, modeling phase, and post-modeling explainability phase. In each phase, different modules work in tandem to provide CSoC analysts with more accurate and explainable output. We believe that this architecture is sufficiently generic to accommodate a variety of scenarios and applications without adhering to a particular specification or technological solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PRE-MODELING PHASE</head><p>The first phase is a pre-modeling phase. The input of this module is raw network flow (dataset) and the output is a highquality dataset. In the following subsections, we will first describe different benchmark datasets available for Intrusion Detection. We then present common data preprocessing techniques used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) DATASETS</head><p>While access to representative, labelled datasets for cybersecurity related AI tasks remains a challenge, a variety of publicly accessible datasets can be used to train and benchmark X-IDS. These are unprocessed network flows extracted from packet captures. To address privacy concerns, many of these datasets are generated in an emulated environment. NSL-KDD <ref type="bibr" target="#b152">[153]</ref>, based on the KDD-CUP-99 <ref type="bibr" target="#b153">[154]</ref>, is a dataset frequently present in the literature. Although old, its use allows comparisons with previous works. NSL-KDD is relatively small compared to other datasets in the field. A more modern dataset is CICIDS2017 <ref type="bibr" target="#b154">[155]</ref>, which contains more up-to-date attacks and network flows. In addition, it includes 3 million samples, which allows scalability testing. Another noteworthy dataset is UGR <ref type="bibr" target="#b155">[156]</ref>, a multi-terabyte dataset collected over the course of 5 months. This dataset is built to test IDS for long-term trends. The authors state that their dataset captures potential trends in daytime, nighttime, weekday, and weekend traffic.</p><p>These publicly available datasets, though good for benchmarking, are not suitable for deployable systems. We recommend, CSoC users deploying X-IDS systems evaluate these systems on organizational representative datasets.</p><p>2) EXPLORATORY DATA ANALYSIS (EDA) AND DATA VISUALIZATION Data preprocessing is essential for increasing the likelihood of ML models producing accurate predictions. Using Exploratory Data Analysis (EDA), one can gain a general understanding of a dataset's key features and characteristics. To comprehend the features, visualization techniques such as heat maps, network diagrams, bar charts, and correlation matrices may be used. Once a comprehension of feature space has been attained, the data is forwarded to the feature engineering model for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) FEATURE ENGINEERING</head><p>The general trend in preprocessing IDS datasets is to normalize the numerical features and to One-Hot Encode (OHE) the categorical features. After the datasets are encoded, their feature space can be quite large which makes them computationally expensive. Two approaches to reducing dimensions are widely discussed in the literature: Feature Selection and Feature Extraction.</p><p>Feature selection techniques are used to reduce the feature space by selecting a subset of features without transforming them. There are three types of feature selection techniques popular in the IDS domain: filters, wrappers, and the embedded/hybrid method <ref type="bibr" target="#b156">[157]</ref>. Apart from these, libraries such as Scikit-Learn <ref type="bibr" target="#b157">[158]</ref> have also been used in published works for feature selection.</p><p>Another technique used in feature engineering is feature extraction (also known as dimensionality reduction). Feature extraction reduces the size of the feature space by transforming the original features while retaining most of their defining attributes. The most commonly used feature extraction technique in the literature is the Principal Component Analysis (PCA) <ref type="bibr" target="#b158">[159]</ref>. PCA is an unsupervised method that does not require class knowledge to identify features. It also facilitates the identification of correlations and relationships between the features of a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MODELING PHASE</head><p>The second phase is the modeling phase. The input of this phase is the high-quality dataset generated in the pre-processing phase and the output is the explanations generated by the explainer module. First, the high-quality dataset is fed into the ML/DL model of choice. Second, the predictions generated by the model in use are passed through an explainer module. Third, these explanations are evaluated by an evaluation module. This process enables users to understand the reason behind certain predictions, which in turn, helps the CSoC analysts in their decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) AI MODEL</head><p>In Section IV, we discussed two different approaches which are currently being employed by different authors to create X-IDS: the black box and the white box. AI modules in these approaches generate predictions. However, there is a trade-off between the accuracy and the interpretability with these approaches. The white box approaches are popular for their interpretability, while the black box approaches are known for their prediction accuracy. In context of IDS, high prediction accuracy is required to prevent attacks. Moreover, black box models can capture significant non-linearity and complex interactions between data that white box models are not able to capture. For example, Recurrent Neural Networks (RNN) can capture temporal dependencies between samples. On the other hand, models like Support Vector Machine (SVM) and Deep Neural Network (DNN) can create their own representation of data. which might be helpful to discover unknown attacks. For this reason, we believe that future X-IDS should be built using black box models.</p><p>In our literature review, we found that authors use a variety of black box algorithms in their work, such as SVM, CNN, RF, and MLP, which prove to be quite effective. Another popular algorithm of choice in the intrusion detection domain is a variant of the RNN, referred to as Long Short-Term Memory (LSTM). Recently, Generative Adversarial Networks (GAN) have also become relatively popular. Consequently, there are a multitude of black box algorithms from which to select. Explainer modules then approximate the prediction generated by AI module employing a white box or black box algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) EXPLAINER MODULE AND EVALUATION</head><p>The prediction generated by the model of choice in the AI module is then fed to the explainer module. The common explainers used from previous works include LIME, SHAP, and LRP. These out-of-the-box modules allow for quick testing on different algorithms and datasets. However, there are some problems with solely using these approaches in future X-IDS works. To begin, methods such as SHAP do not run in real-time. Therefore, it may be time-consuming to attempt to use SHAP on a simple Multi-Layer Perceptron classifier with a large feature space dataset. In X-IDS, both predictions and explanations must be made as quickly as possible. Secondly, these approaches are not always designed with X-IDS stakeholders in mind.</p><p>At present, there are no set standard metrics to evaluate explanations. Several authors have attempted to evaluate explanations in various ways. In Section II-C we described different ways to evaluate the explanations. Metrics such as application grounded evaluation, human-grounded evaluations, and function-grounded evaluation proposed by Doshi et al. <ref type="bibr" target="#b80">[81]</ref> can be used as a baseline to evaluate the explanation generated by X-IDS. A noteworthy method to evaluate the effectiveness of explanations is proposed by authors in <ref type="bibr" target="#b23">[24]</ref>. Figure <ref type="figure">6</ref> illustrates their approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. POST MODELING EXPLAINABILITY PHASE</head><p>The third and final phase is the post-modeling explainability phase. This phase has two major components: the explanation interface and users. The recommendation, decision, or action generated by the AI module, explained by an explainer module, and evaluated by an explanation evaluation module is rendered in a graphical user interface (explanation interface). The users, on the other hand, use this interface to make an informed decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) EXPLANATION INTERFACE</head><p>The custom visual dashboards are created to help the user to understand the X-IDS. An excellent approach to building such an explanation interface is found in the work by <ref type="bibr" target="#b101">[102]</ref> and <ref type="bibr" target="#b97">[98]</ref>. The engineers who design X-IDS can use this approach as guidance to create their explanation interface. Furthermore, this paper also recommends that future X-IDS developers make custom explainers built for specific stakeholders to help improve explainability. Open-source toolkits and libraries are also available that create a visual dashboard and explain the prediction. One such library is Shapash <ref type="bibr" target="#b159">[160]</ref>. It is an overlay package to other intelligibility libraries, such as Shap and Lime, that are dedicated to the interpretability of models. Another example of the library for quickly building interactive dashboards for analyzing and explaining the predictions and workings of sci-kit-learn machine learning models is explainerdashboard <ref type="bibr" target="#b160">[161]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) USERS</head><p>For this paper, the stakeholders will consist of developers, defense practitioners, and investors. Section VI-B discusses the need for defining the identity of the stakeholders of an X-IDS system. The developers are tasked with creating, modifying, and maintaining the X-IDS. The defense practitioners guard the assets of the investors. Lastly, the investors make budgeting decisions for the benefit of the X-IDS system and other assets. These three audiences have distinct tasks and explainability requirements that must be addressed differently by the X-IDS. An explanation interface designed from the user's perspective can bridge this gap.</p><p>If an explanation is unclear or unhelpful, the stakeholders will need a way to voice that opinion. For example, a set of explanations is too complicated for a group of investors. Investors may ask for additional explanations that simplify or even link to a web page that can teach them more on a subject. In such a situation, the developers can revise the explainer module to fit the users request or needs. This could also include making a new explainer module or updating to a new state-of-the-art module like those in AIX360 <ref type="bibr" target="#b79">[80]</ref>. For the same reasons, incorrect predictions and explanations need to be corrected and updated. The developers or defense practitioners will then need to introduce new data to the model. Moreover, a different method of data preprocessing may be required to augment the efficacy of the model.</p><p>To make the recommended X-IDS architecture as shown in Figure <ref type="figure">4</ref> a reality, researchers need to study different aspects of the three phases. To this end, in the next section we discuss various challenges inherent in designing the proposed X-IDS architecture and make research recommendations aimed at effectively mitigating these challenges for future researchers interested in developing X-IDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESEARCH CHALLENGES AND RECOMMENDATIONS</head><p>The sub-domain of explainable AI based Intrusion Detection Systems is still in its infancy. Researchers working on X-IDS must be made aware of the issues that hinder its development. The issues that we described in Section II such as finding the right notion of explainability, generating explanations from a stakeholder's perspective, and lack of formal standard metrics to evaluate explanations are prevalent in the X-IDS domain as well. Existing X-IDS research is primarily focused on the goal of making algorithms explainable.</p><p>Explanations are not being designed around stakeholders, and researchers need to quantify useful evaluation metrics. Apart from these challenges, issues pertaining to IDS may also pose a problem for X-IDS. There are many promising avenues of exploration, in this section we detail some existing research challenges and give our recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DEFINING EXPLAINABILITY FOR INTRUSION DETECTION</head><p>The first problem faced by researchers designing X-IDS is the lack of consensus on the definition of explainability in IDS. The research community needs to agree on a common definition of explainability for IDS. To find common ground, we can leverage the foundational XAI definition proposed by DARPA <ref type="bibr" target="#b30">[31]</ref>. However, an X-IDS definition needs more security domain-specific elements. The inclusion of the CIA principles may be a good start for cementing a definition that combines aspects of cybersecurity and XAI.</p><p>Questions relevant to the X-IDS that researchers need to answer include: ''What is explainability when used for intrusion detection?'', ''How do we effectively create explanations for IDS?'', and ''Who are we creating explanations for?''. Other questions such as ''How can Confidentiality, Integrity, and Availability benefit from explanations?'' and ''How do we categorize X-IDS algorithms?'' should be reassessed by X-IDS researchers as well. Current work is extremely narrow in its scope and limits its objective to explaining each sample in an IDS dataset. These works also do not consider the type of audience when building X-IDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DEFINING TASKS AND STAKEHOLDERS</head><p>The second challenge is to define the task and the stakeholders of the X-IDS ecosystem. After formalizing the definition of 'explainability' for X-IDS, we need to create explanations tailored to the stakeholders. Figure <ref type="figure" target="#fig_4">5</ref> demonstrates a simple user and explanation taxonomy. We consider three major stakeholders based on their roles in this taxonomy including IDS developers, security analyst, and investors. Each of the stakeholder categories necessitates a different degree of explanation and visualization. Developers and CSoC members are more familiar with the field and may want more complex explanations. Investors and managers, on the other hand, may be more satisfied with summarized visualizations. Each user group performs varying tasks based on the explanations. Programmers will work to debug and increase the efficacy of the AI model. CSoC members will be tasked with protecting investor assets. Indirectly, investors will need to make hiring or budgeting decisions. Take for example a corporate, network security system. The set of stakeholders consists of IDS developers, security analysts, upper-level managers, and team managers. In such a scenario, IDS developers will be creating and/or updating the corporation's IDS. These developers will want the IDS to return which features from local and global explanations are making the most impact. Additionally, as attacks change over time, these</p><p>VOLUME 10, 2022 individuals will want explanations potentially keeping track of these changes. New attacks can be known to exist if the performance begins to degrade, so accuracy and performance metrics will be integral to maintaining the IDS. Security analyst or server admins would benefit in a similar way to IDS developers. Having potential leads to base their actions on could lower system down time. The different managerial levels could be using certain other metrics or explanations to aid them in leadership decisions. Hiring more staff, adding more funding, or deciding to pivot to a new system would be some actions managers could take. The needs of each group are different and future research is needed to determine the best types of explanations that will benefit each group the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EVALUATION METRICS</head><p>The third challenge in designing X-IDS is evaluating the explanation generated by the 'explainer module'. Finding the best explanation for each stakeholder category requires customized evaluation metrics. Currently, there is no consensus on metrics for explanations. In Section II-C we described a body of literature proposing various evaluation metrics that could be used towards evaluating explanations. In particular, we recommended evaluation metrics proposed by authors in <ref type="bibr" target="#b80">[81]</ref> to evaluate explanations for X-IDS in Section V-B. Another notable work that could serve as a baseline for evaluating explanations is the psychological model of explanation created by the Florida Institute for Human and Machine Cognition (IHMC) <ref type="bibr" target="#b23">[24]</ref>. The proposed model is illustrated in Figure <ref type="figure">6</ref>. The user receives an explanation from the XAI model. This explanation can be tested for ''goodness'' and the satisfaction of the user/stakeholder. The user then revises their mental model of the XAI system. Their understanding of the system can be tested. Tasks are performed based on the explanation. The IHMC model merges the purpose of the XAI model, with the task and mindset of the user.A noteworthy method to evaluate the effectiveness of explanations is proposed by authors in <ref type="bibr" target="#b23">[24]</ref>. Figure <ref type="figure">6</ref> illustrates their approach. FIGURE 6. Different categories for assessing the effectiveness of explanations in the IHCM psychological model with detailed explanation process <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ADVERSERIAL AI</head><p>Adversarial AI refers to the use of artificial intelligence for malicious purposes, including attacks on other artificial intelligence systems to evade detection <ref type="bibr" target="#b161">[162]</ref> or to poison data <ref type="bibr" target="#b162">[163]</ref>. Malicious actors can potentially attack the classifiers that are used to generate predictions and cause misclassification. In context of X-IDS, the explanations generated by the explainer module may become a new point of attack for malicious actors. Attackers may add, delete, or modify explanations to evade detection <ref type="bibr" target="#b163">[164]</ref>. Attackers may also attack training datasets to alter the explainer's behavior. The methods and effects of these attacks will need to be explored. Defense techniques must be created to correct attacked explanations. Studies to defend IDS against adversarial attack include <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b165">[166]</ref>, and <ref type="bibr" target="#b166">[167]</ref>, etc. Study-specific to adversarial approach for X-IDS is discussed in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. MISLEADING/INCORRECT EXPLANATIONS</head><p>An explanation does not have to be attacked to be misleading. The explanation itself may be misleading, or the user may interpret the explanation incorrectly. This may lead to circumstances where the model is correct and the user is the problem. The explainer will need to be modified to prevent user error in such situations.</p><p>Explanations that are misclassified either by an attack or due to the poor quality of data can have a significant negative impact on CSoCs. CSoCs security analysts should always critically analyze the reasoning behind the prediction. Moreover, methods for auditing previously incorrect explanations should be created. Ideally, the X-IDS should be able to audit itself and generate explanations for the audit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. SCALABILITY AND PERFORMANCE</head><p>Performance is of utmost importance for an IDS. CSoCs can incur losses for lost time. Explanations should not needlessly slow down an IDS. So how do we optimize an X-IDS? One approach is that the explainer could generate explanations for every sample it sees, or it could strategically choose which samples to explain. A comprehensive analysis of the CPU, RAM, and disk usage should be run on current and future explainers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>The exponential growth of cyber networks and the myriad applications that run on them have made CSoC, Cyberphysical systems, and critical infrastructure vulnerable to cyber-attacks. Securing these domains and their resources through the use of defense tools such as IDS, is critical to combating and resolving this issue <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Recent AI-based IDS research has demonstrated unprecedented prediction accuracy, which is helping to lead to its widespread adoption across the industry. CSoC analysts largely rely on the results of these models to make their decision. However, in most cases, decision-making is impaired simply because these opaque models fail to justify their predicted outcomes. A solution to this problem is to embrace the concept of 'explainability' in these models. This, in turn, may facilitate quick interpretation of prediction, making it more feasible for CSoC analysts to accelerate response times.</p><p>A systematic review of current state-of-the-art research on 'XAI' or 'explainability' highlighted some key challenges in this domain, such as the lack of consensus surrounding the definition of 'explainability', the need to formalize explainability from the user's perspective, and the lack of metrics to evaluate explanations. We propose a taxonomy to address this problem with a focus on its relevance and applicability to the domain of intrusion detection.</p><p>In this paper, we present in detail two distinct approaches found in the body of literature which address the concern of 'explainability' in the IDS domain, including the white box approach and the black box approach. The white box approach makes the model in use inherently interpretable, whereas the black box approach requires post-hoc explanation techniques to make the predictions more interpretable (e.g., LIME <ref type="bibr" target="#b70">[71]</ref>, SHAP <ref type="bibr" target="#b31">[32]</ref>).While the former approach may provide a more detailed explanation to assist CSoC members in decision-making, its prediction performance is in general outperformed by the latter. Nevertheless, the field of IDS requires a high degree of precision to prevent attacks and avoid false positives. Bearing this in mind, a black box approach is recommended when developing an X-IDS solution.</p><p>In addition, we also propose a three-layered architecture for the design of an X-IDS based on the DARPA recommended architecture <ref type="bibr" target="#b23">[24]</ref> for the design of XAI systems. This architecture is sufficiently generic to support a wide variety of scenarios and applications without being bound by a particular specification or technological solution. Finally, we provide research recommendations to researchers that are interested in developing X-IDS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1.A taxonomic approach to explainability definition based on explainability concepts, formalizing explainability tasks from the standpoint of stakeholders, and evaluating explainability techniques. Green represents model dependency, while grey represents the scope of the explanations. Light purple represents various types of stakeholders in the IDS ecosystem. Pink represents techniques to evaluate explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. An overview of our proposed taxonomy. We categorize X-IDS techniques into two families, white box and black box. White box approaches encompass the techniques of Regression, Rule-Based, Clustering, and Statistical &amp; Probabilistic Methods. Black box approaches encompass Feature, Perturbation, Decomposition, and Hybrid approaches. These approaches define the method of explainability to interpret the model's decision process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. A visual depiction of Layer-wise Relevance Propagation.Relevance scores (R j , R k ) are calculated backwards from the output for each layer (j and k represent neurons). Scores from each previous layer are used to score the next set of neurons with the final outcome being the importance of each input<ref type="bibr" target="#b109">[110]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. A simple taxonomy illustrating the importance of tailoring explanations to specific stakeholders based on their roles in CSoCs.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 10, 2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>• We present the state-of-the-art of the XAI approach and discuss the critical issues that surround it, mostVOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112394" xml:id="foot_2"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112396" xml:id="foot_4"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112398" xml:id="foot_5"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112400" xml:id="foot_6"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112402" xml:id="foot_7"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112404" xml:id="foot_8"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112406" xml:id="foot_9"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112408" xml:id="foot_10"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112410" xml:id="foot_11"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112412" xml:id="foot_12"><p>  VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112414" xml:id="foot_13"><p>  VOLUME 10, 2022   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the <rs type="institution">U.S. Army ERDC</rs> or the <rs type="institution">U.S. DoD</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported by <rs type="funder">Mississippi State University</rs> through the <rs type="funder">U.S. Department of Defense (DoD) High Performance Computing Modernization Program</rs> through the <rs type="funder">U.S. Army Engineering Research and Develop Center (ERDC)</rs> under Grant <rs type="grantNumber">W912HZ-21-C0058</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jC4NGbF">
					<idno type="grant-number">W912HZ-21-C0058</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SUBASH NEUPANE received the bachelor's degree in computer engineering from Kathmandu University, Nepal, in 2011, the M.S. degree in information technology (professional computing) from the Swinburne University of Technology, Melbourne, Australia, in 2016, and the M.S. degree in information systems and security management from Tuskegee University, AL, USA, in 2020. He is currently pursuing the Ph.D. degree in systems and security with Mississippi State University. He was a Telecom Network Engineer with Tandem Corporation, Melbourne, before moving to the USA. His current research interests include systems and security, machine learning, and blockchain. Her experience has included work with ocean modeling, underwater seismic data collection and processing, geographical information systems design, natural language processing, and machine learning. At ERDC, she has been involved with research in making scalable machine learning algorithms available on high performance computing platforms and expanding the laboratory's capabilities to manage and analyze very large data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JESSE ABLES (Graduate</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of deep learning-based network anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="949" to="961" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cyber-physical system security for the electric power grid</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Govindarasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="224" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cyber-physical systems: The next computing revolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 47th Design Autom. Conf. (DAC)</title>
		<meeting>47th Design Autom. Conf. (DAC)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="731" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Challenges for securing cyber physical systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinopoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Workshop Future Directions Cyber-Phys. Syst. Secur.</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information leakage in cloud data warehouses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Marinescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sustain. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="203" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adversarial approach for explainable AI in intrusion detection systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wickramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Annu. Conf. IEEE Ind</title>
		<meeting>44th Annu. Conf. IEEE Ind</meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="3237" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An intrusion response system utilizing deep Q-networks and system partitions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cardellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iannucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucantonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silvi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08182</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantically rich framework to automate cyber insurance services</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSC.2021.3113272</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput., early access</title>
		<imprint>
			<date type="published" when="2021-09-16">Sep. 16, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning techniques for behavioural malware analysis in cloud iaas,&apos;&apos; in Malware Analysis Using Artificial Intelligence and Deep Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcdole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alazab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explainable AI and random forest based reliable intrusion detection system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An explainable machine learning framework for intrusion detection systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="73127" to="73141" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">Cyber Security Operations Center (CSOC), Raytheon</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computer security threat monitoring and surveillance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An intrusion-detection model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="1987-02">Feb. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intrusion detection systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Bace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The use of computational intelligence in intrusion detection systems: A review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Network intrusion detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Heberlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A data mining framework for building intrusion detection models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Mok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Secur. Privacy</title>
		<meeting>IEEE Symp. Secur. Privacy</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="120" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of data mining and machine learning methods for cyber security intrusion detection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Buczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1153" to="1176" />
			<date type="published" when="2015">2015</date>
			<pubPlace>2nd Quart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance evaluation of intrusion detection based on machine learning using apache spark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">El</forename><surname>Hadaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Idhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning in intrusion detection system: An overview,&apos;</title>
		<author>
			<persName><forename type="first">E</forename><surname>Aminanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">&apos; in Proc. Int. Res. Conf. Eng. Technol. (IRCET)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning in intrusion detection perspective: Overview and further challenges</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Aminanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Big Data Inf. Secur. (IWBIS)</title>
		<meeting>Int. Workshop Big Data Inf. Secur. (IWBIS)</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explainable AI: A brief survey on history, research areas, approaches and challenges</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. CCF Int. Conf. Natural Lang. Process. Chin. Comput</title>
		<imprint>
			<biblScope unit="page" from="563" to="574" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DARPA&apos;s explainable artificial intelligence (XAI) program</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08296</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Artificial intelligence: Explainability, ethical issues and bias</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Robot. Autom</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="2021-08">Aug. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical procedures for forecasting criminal behavior: A comparative assessment</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Criminol. Pub. Pol&apos;y</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">513</biblScope>
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GEE: A gradient-based explainable variational autoencoder for network anomaly detection</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An explainable artificial intelligence system for small-unit tactical behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Lent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancuso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Nat. Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="900" to="907" />
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Menlo Park, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<idno>DARPA-BAA-16-53</idno>
	</analytic>
	<monogr>
		<title level="m">Broad Agency Announcement Explainable Artificial Intelligence (XAI)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst.</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving user trust on deep neural networks based intrusion detection systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Annu. Conf. IEEE Ind</title>
		<meeting>44th Annu. Conf. IEEE Ind</meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="3262" to="3268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explanations and expectations: Trust building in automated vehicles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haspiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tilbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Companion ACM/IEEE Int. Conf. Hum.-Robot Interact</title>
		<meeting>Companion ACM/IEEE Int. Conf. Hum.-Robot Interact</meeting>
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explaining deep learning-based driver models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P S</forename><surname>Lorente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Florez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Espino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A I</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>De Miguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3321</biblScope>
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deep learning-based hybrid framework for object detection and recognition in autonomous driving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="194228" to="194239" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interpretable global-local dynamics for the prediction of eye fixations in autonomous driving scenarios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez-Cebrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Fernandez-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diaz-De-Maria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="217068" to="217085" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Identification and explanation of challenging conditions for camera-based object detection of automated vehicles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ponn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diermeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">3699</biblScope>
			<date type="published" when="2020-07">Jul. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The judicial demand for explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Columbia Law Rev</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1829" to="1850" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding the criminal behavior in Mexico City through an explainable artificial intelligence model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Loyola-González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Mexican Int. Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="136" to="149" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An explainable multi-attribute decision model based on argumentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="42" to="61" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A method for explaining Bayesian networks for legal evidence with scenarios</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Vlek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Verheij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Law</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="324" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">What do we need to build explainable AI systems for the medical domain?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09923</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explainable AI in industry</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="page" from="3203" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A one-class classification decision tree based on kernel density estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lecron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fortemps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">106250</biblScope>
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for falls prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moorhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adv. Comput. Data Sci</title>
		<meeting>Int. Conf. Adv. Comput. Data Sci</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Explainable machine learning framework for image classification problems: Case study on glioma cancer prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pintelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liaskos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Livieris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interpretable and accurate prediction models for metagenomics data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Prifti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chevaleyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Belda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Danchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Explainable machine learning predictions to help anesthesiologists prevent hypoxemia during surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Vavilala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eisses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Liston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2017-01">Jan. 2017. 206540</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quantitative structure-mutation-activity relationship tests (QSMART) model for protein kinase inhibitor response prediction</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">EXplainable artificial intelligence (XAI) for the identification of biologically relevant gene expression patterns in longitudinal human studies, insights from obesity research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anguita-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segura-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alcalá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alcalá-Fdez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
	<note>Art. no. e1007792</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Expert level evaluations for explainable ai (XAI) methods in the medical domain</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Muddamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Jahromi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
		<meeting>Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Concept attribution: Explaining CNN decisions to physicians</title>
		<author>
			<persName><forename type="first">M</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">103865</biblScope>
			<date type="published" when="2020-08">Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding the decisions of CNNs: An in-model approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rio-Torto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Study on credit rating model using explainable AI</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Korean Data Inf. Sci. Soc</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="295" />
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint banknote recognition and counterfeit detection using explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">3607</biblScope>
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">To trust or not to trust an explanation: Using LEAF to evaluate local linear XAI methods</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amparore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bajardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Evaluating XAI: A comparison of rule-based and example-based explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Waa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nieuwburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neerincx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
	<note>Art. no. 103404</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Explainability fact sheets: A framework for systematic assessment of explainable approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Fairness, Accountability, Transparency</title>
		<meeting>Conf. Fairness, Accountability, Transparency</meeting>
		<imprint>
			<date type="published" when="2020-01">Jan. 2020</date>
			<biblScope unit="page" from="56" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On explainable fuzzy recommenders and their performance evaluation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Łapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nielek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Math. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="610" />
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Personalized reason generation for explainable song recommendation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Explainable AI in industry: Practical challenges and lessons learned</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Companion Proc. Web Conf</title>
		<meeting>Companion . Web Conf</meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="page" from="303" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Notions of explainability and evaluation approaches for explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vilone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="89" to="106" />
			<date type="published" when="2021-12">Dec. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Opportunities and challenges in explainable artificial intelligence (XAI): A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11371</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Explainable AI: A review of machine learning interpretability methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Linardatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papastefanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Why should i trust you?&apos; explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. AAAI Conf. Artif. Intell.</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Distribution-free predictive inference for regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1094" to="1111" />
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Int. Conf. Artif. Neural Netw</title>
		<imprint>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV),&apos;</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">&apos; in Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="2668" to="2677" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Global model interpretation via recursive partitioning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Conf. High Perform. Comput. Communications; IEEE 16th Int. Conf. Smart City; IEEE 4th Int. Conf. Data Sci. Syst</title>
		<meeting>IEEE 20th Int. Conf. High Perform. Comput. Communications; IEEE 16th Int. Conf. Smart City; IEEE 4th Int. Conf. Data Sci. Syst</meeting>
		<imprint>
			<publisher>HPCC/SmartCity/DSS</publisher>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="1563" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Lightlysupervised representation learning with global interpretability</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Valenzuela-Escárcega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11545</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Explainability in human-agent systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auto. Agents Multi-Agent Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="673" to="705" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">One explanation does not fit all: A toolkit and taxonomy of AI explainability techniques</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pedemonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03012</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 5th Int. Conf. Data Sci</title>
		<meeting>IEEE 5th Int. Conf. Data Sci</meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Generating contrastive explanations with monotonic attribute functions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A survey of intrusion detection systems in wireless sensor networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Butun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Morgera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="266" to="282" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>1st Quart.</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Evolution and detection of polymorphic and metamorphic malwares: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.7061</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view</title>
		<author>
			<persName><forename type="first">O</forename><surname>Loyola-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154096" to="154113" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Visualizing the effects of predictor variables in black box supervised learning models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc., B, Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1059" to="1086" />
			<date type="published" when="2020-09">Sep. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Statist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="65" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Predictive learning via rule ensembles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="916" to="954" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">RISE: Randomized input sampling for explanation of black-box models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07421</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Not just a black box: Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01713</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep Taylor decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Domain knowledge aided explainable artificial intelligence for intrusion detection and response</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09853</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">An explainable machine learning-based network intrusion detection system for enabling generalisability in securing IoT networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Layeghy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Portmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07183</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Explaining anomalies detected by autoencoders using SHAP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Antwarg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02407</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A new explainable deep learning framework for cyber threat discovery in industrial IoT networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Sallam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="11604" to="11613" />
			<date type="published" when="2022-07">Jul. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Feature-oriented design of visual analytics system for interpretable deep learning based intrusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Theor</title>
		<meeting>Int. Symp. Theor</meeting>
		<imprint>
			<date type="published" when="2020-12">Dec. 2020</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Explanation framework for intrusion detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Burkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Cyber Physical Systems</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Toward explainable deep neural network based anomaly detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Hum. Syst. Interact. (HSI)</title>
		<meeting>11th Int. Conf. Hum. Syst. Interact. (HSI)</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="311" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Towards explaining anomalies: A deep Taylor decomposition of one-class models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06230</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Achieving explainability of intrusion detection system by hybrid oracle-explainer approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szczepanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pawlicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kozik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw. (IJCNN)</title>
		<meeting>Int. Joint Conf. Neural Netw. (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul. 2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Explainable deep few-shot anomaly detection with deviation networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00462</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Understanding the decision of machine learning based intrusion detection systems</title>
		<author>
			<persName><forename type="first">Q.-V</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Future Data Secur. Eng</title>
		<meeting>Int. Conf. Future Data Secur. Eng</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="379" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Explaining recurrent neural network predictions in sentiment analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07206</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation: An overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Leveraging grad-CAM to improve the accuracy of network intrusion detection systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Caforio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andresini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Appice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Malerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discovery Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">The clever Hans effect in anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10609</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Interpretability in intelligent systems-A new concept?&apos;&apos; in Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rieger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A grey-box ensemble model exploiting black-box accuracy and white-box intrinsic interpretability</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pintelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Livieris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence approaches: A survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09429</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Explainable machine learning for intrusion detection via hardware performance counters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Kuruvila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Basu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2022.3149745</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., early access</title>
		<imprint>
			<date type="published" when="2022-07">Feb. 7, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (XAI) to enhance trust management in intrusion detection systems using decision tree model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mahbooba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Timilsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sahal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">A hybrid approach for an interpretable and explainable intrusion detection system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Praça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sousa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10280</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A new method for flow-based network intrusion detection using the inverse Potts model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F T</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J C</forename><surname>Gondim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Netw. Service Manage</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1125" to="1136" />
			<date type="published" when="2021-06">Jun. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Explainable unsupervised machine learning for cyberphysical systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wickramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="131824" to="131843" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">ANNaBell Island: A 3D color hexagonal SOM for visual intrusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Langin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Sci. Inf. Secur</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Intrusion detection systems using linear discriminant analysis and logistic regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karmakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. IEEE India Conf. (INDICON)</title>
		<meeting>Annu. IEEE India Conf. (INDICON)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A new anomaly detection method based on IGTE and IGFE</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Secur. Privacy Commun. Netw</title>
		<meeting>Int. Conf. Secur. Privacy Commun. Netw</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="93" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Robust regression for anomaly detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Commun. (ICC)</title>
		<meeting>IEEE Int. Conf. Commun. (ICC)</meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">An explainable artificial intelligence model for clustering numerical databases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Loyola-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gutierrez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Medina-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez-Trinidad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Carrasco-Ochoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garcia-Borroto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="52370" to="52384" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">ExKMC: Expanding explainable K -means clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moshkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02399</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Explainable K-means and K-medians clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moshkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Int. Conf. Mach. Learn</title>
		<meeting>37th Int. Conf. Mach. Learn<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">An application of machine learning to network intrusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu</title>
		<meeting>15th Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="371" to="377" />
		</imprint>
	</monogr>
	<note>Conf. (ACSAC)</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Genetic algorithm rule-based intrusion detection system (GAIDS)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ojugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eboka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Okonta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aghware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Emerg. Trends Comput. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1182" to="1194" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Hybrid genetic fuzzy rule based inference engine to detect intrusion in networks,&apos;&apos; in Intelligent Distributed Computing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Snort: Lightweight intrusion detection for networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. LISA</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Snort intrusion detection prevention toolkit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syngress, Tech. Rep</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Taxonomy of statistical based anomaly detection techniques for intrusion detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Qayyum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
		<meeting>IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
			<biblScope unit="page" from="270" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Survey of intrusion detection systems: Techniques, datasets and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khraisat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamruzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybersecurity</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Network anomaly detection: Methods, systems and tools</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="303" to="336" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>1st Quart.</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Network anomaly detection,&apos;&apos; in Future Internet (FI) and Innovative Internet Technologies and Mobile Communication (IITM) Focal Topic: Advanced Persistent Threats</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">An informationtheoretic combining method for multi-classifier anomaly detection systems,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashfaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Khayam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Commun</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A multi-order Markov chain based scheme for anomaly detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 37th Annu</title>
		<meeting>IEEE 37th Annu</meeting>
		<imprint>
			<date type="published" when="2013-07">Jul. 2013</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A Markov chain model of temporal behavior for anomaly detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Syst., Man, Cybern. Inf. Assurance Secur. Workshop</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Engineering applications of the self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1358" to="1384" />
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A computer host-based user anomaly detection system using the self-organizing map</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hatonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Sorvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE-INNS-ENNS Int. Joint Conf. Neural Netw</title>
		<meeting>IEEE-INNS-ENNS Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Hostbased intrusion detection using self-organizing maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichodzijewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Heywood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw. (IJCNN)</title>
		<meeting>Int. Joint Conf. Neural Netw. (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="1714" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A hierarchical SOM-based intrusion detection system</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Kayacik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Heywood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="451" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A model to use denied internet traffic to indirectly discover internal network security problems,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">C</forename><surname>Langin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Perform., Comput. Commun. Conf</title>
		<imprint>
			<biblScope unit="page" from="486" to="490" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A self-organizing map and its modeling for discovering malignant network traffic</title>
		<author>
			<persName><forename type="first">C</forename><surname>Langin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zargham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sayeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
		<meeting>IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2009-03">Mar. 2009</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Method of intrusion detection using deep neural network,&apos;&apos; in</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Big Data Smart Comput. (BigComp)</title>
		<meeting>IEEE Int. Conf. Big Data Smart Comput. (BigComp)</meeting>
		<imprint>
			<date type="published" when="2017-02">Feb. 2017</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Detecting anomalies in robot time series data using stochastic recurrent networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sölch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A deep learning approach for intrusion detection using recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="21954" to="21961" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">1D CNN based network intrusion detection with normalization on imbalanced data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azizjon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jumabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2020-02">Feb. 2020</date>
			<biblScope unit="page" from="218" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Applying convolutional neural network for network intrusion detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vinayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poornachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adv. Comput</title>
		<meeting>Int. Conf. Adv. Comput</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="1222" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">From machine learning to explainable AI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Symp</title>
		<meeting>World Symp</meeting>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">FAIXID: A framework for enhancing AI explainability of intrusion detection results using data cleaning techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alnusair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Netw. Syst. Manage</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2021-10">Oct. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A detailed analysis of the KDD CUP 99 data set</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tavallaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
		<meeting>IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2009-07">Jul. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<ptr target="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" />
		<title level="m">KDD Cup 1999 Data the UCI KDD Archive</title>
		<imprint>
			<date type="published" when="1999-04">1999. Apr. 9, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Toward generating a new intrusion detection dataset and intrusion traffic characterization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sharafaldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf</title>
		<meeting>4th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">UGR-16: A new dataset for the evaluation of cyclostationarity-based network IDSs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Maciá-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Magán-Carrión</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>García-Teodoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Therón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Secur</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="411" to="424" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">A survey of feature selection and feature extraction techniques in machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nasreen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Sci. Inf. Conf</title>
		<imprint>
			<biblScope unit="page" from="372" to="378" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">LIII. On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R S K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">London, Edinburgh, Dublin Philosoph. Mag. J. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<ptr target="https://shapash.readthedocs.io/en/latest/overview.html" />
		<title level="m">Shapash Welcome to Shapash&apos;s Documentation</title>
		<imprint>
			<date type="published" when="2022-07-23">Jul. 23, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">ExplainerDashboard Starting the Default Dashboard</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dijk</surname></persName>
		</author>
		<ptr target="https://explainerdashboard.readthedocs.io/en/latest/dashboards.html" />
		<imprint>
			<date type="published" when="2019-07-22">2019. Jul. 22, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">DeepDGA: Adversariallytuned domain generation and detection,&apos;</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Filar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">&apos; in Proc. ACM Workshop Artif. Intell. Secur</title>
		<imprint>
			<biblScope unit="page" from="13" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Explaining RADAR features for detecting spoofing attacks in connected autonomous vehicles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rampazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levitt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00150</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Evaluating and improving adversarial robustness of machine learningbased network intrusion detectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2632" to="2647" />
			<date type="published" when="2021-08">Aug. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Defending network intrusion detection systems against adversarial evasion attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pawlicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choraś</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kozik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="148" to="154" />
			<date type="published" when="2020-09">Sep. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Explainability and adversarial robustness for RNNs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hartl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bachl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fabini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zseby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 6th Int. Conf. Big Data Comput</title>
		<meeting>IEEE 6th Int. Conf. Big Data Comput</meeting>
		<imprint>
			<date type="published" when="2020-08">Aug. 2020</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
