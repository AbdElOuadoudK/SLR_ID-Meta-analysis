<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A GAN and Feature Selection-Based Oversampling Technique for Intrusion Detection</title>
				<funder ref="#_4HNUVuK">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_6SVKFcx #_Z55aXDh">
					<orgName type="full">National Natural Science of Foundation of China</orgName>
				</funder>
				<funder ref="#_UnmVB5w">
					<orgName type="full">Beijing Excellent Talent Funding-Youth Project</orgName>
				</funder>
				<funder ref="#_vcZKgDM">
					<orgName type="full">CCF-NSFocus Funding</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-06">6 July 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0002-5307-8165</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tong</forename><surname>Li</surname></persName>
							<email>litong@bjut.edu.cn</email>
							<idno type="ORCID">0000-0002-8881-0037</idno>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Engineering Research Center of Intelligent Perception and Autonomous Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Runzi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NSFOCUS Technologies Group Co., Ltd</orgName>
								<address>
									<postCode>100089</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100089</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0000-0003-2092-8650</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongheng</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0003-4314-0065</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Engineering Research Center of Intelligent Perception and Autonomous Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A GAN and Feature Selection-Based Oversampling Technique for Intrusion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-06">6 July 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">C7C040414B53B237B50ACED076375EDD</idno>
					<idno type="DOI">10.1155/2021/9947059</idno>
					<note type="submission">Received 5 April 2021; Revised 10 June 2021; Accepted 26 June 2021;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-02T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, there have been numerous cyber security issues that have caused considerable damage to the society. (e development of efficient and reliable Intrusion Detection Systems (IDSs) is an effective countermeasure against the growing cyber threats. In modern high-bandwidth, large-scale network environments, traditional IDSs suffer from a high rate of missed and false alarms. Researchers have introduced machine learning techniques into intrusion detection with good results. However, due to the scarcity of attack data, such methods' training sets are usually unbalanced, affecting the analysis performance. In this paper, we survey and analyze the design principles and shortcomings of existing oversampling methods. Based on the findings, we take the perspective of imbalance and high dimensionality of datasets in the field of intrusion detection and propose an oversampling technique based on Generative Adversarial Networks (GAN) and feature selection. Specifically, we model the complex high-dimensional distribution of attacks based on Gradient Penalty Wasserstein GAN (WGAN-GP) to generate additional attack samples. We then select a subset of features representing the entire dataset based on analysis of variance, ultimately generating a rebalanced low-dimensional dataset for machine learning training. To evaluate the effectiveness of our proposal, we conducted experiments based on the NSL-KDD, UNSW-NB15, and CICIDS-2017 datasets. (e experimental results show that our method can effectively improve the detection performance of machine learning models and outperform the baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>(e rapid development of network technology has dramatically improved people's daily lives, but it has also brought many threats. For example, Marriott International's Starwood network was maliciously breached, and the private information of some 500 million users was compromised (<ref type="url" target="http://sn.people.com.cn/n2/2018/1204/c190199-32363619.html">http://sn.people.  com.cn/n2/2018/1204/c190199-32363619.html</ref>). Not only is there a risk of personal information being copied on the Internet, but corporate production is also under serious threat. In 2018, Taiwan Semiconductor Manufacturing Corporation (TSMC), the world's number one chip foundry, was compromised by the WannaCry ransomware virus, which led to a complete shutdown of all production lines and ultimately caused losses of approximately NTD 5.2 billion (<ref type="url" target="https://english.cw.com.tw/article/article.action?id=2194">https://english.   cw.com.tw/article/article.action?id=2194</ref>). (e 2018-2019 Global Application and Cyber Security Report released by Radware shows that 93% of respondents have suffered from network attacks in the past 12 months. Network security has become an issue that people cannot ignore.</p><p>Intrusion Detection Systems (IDSs) have been widely adopted as an effective method to detect and defend against network attacks in response to the growing network threats. It monitors network traffic in real-time, divides network records into normal records and malicious records, and provides essential information for the defense system. In the last few decades, machine learning has been used to improve intrusion detection <ref type="bibr" target="#b0">[1]</ref>. Nevertheless, due to the sparsity of attack data, the training set for this type of approach is unbalanced, affecting analysis performance <ref type="bibr" target="#b1">[2]</ref>.</p><p>Oversampling techniques are commonly used to address the problem of unbalanced datasets. Traditional methods are used to generate samples among the nearest neighbors by interpolation, such as Synthetic Minority Oversampling Technique (SMOTE) <ref type="bibr" target="#b2">[3]</ref>, and Adaptive Synthetic Sampling Technique (ADASYN) <ref type="bibr" target="#b3">[4]</ref>. Generative Adversarial Network (GAN) is a new generative model that provides a new framework for sample generation <ref type="bibr" target="#b4">[5]</ref>. It allows the generator to learn the data features sufficiently by gaming between the generator and the discriminator to simulate data distributions. It shows its most advanced technology in the generation of images, sounds, and texts <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Moreover, researchers in other fields are regularly applying this method in their research direction.</p><p>(is paper presents an oversampling technique based on Generative Adversarial Networks (GAN) and Feature Selection (GAN-FS) applied to intrusion detection from the perspective of data imbalance and high dimensionality. We construct an attack sample generation model based on an improved generative adversarial network WGAN-GP. In addition, considering the characteristics of large data volume and high dimensionality in intrusion detection, we use Analysis of Variance (ANOVA) for data dimensionality reduction. Effective data dimensionality reduction can remove redundant and irrelevant features to reduce the curse of dimensionality and thus improve classification accuracy <ref type="bibr" target="#b8">[9]</ref>. Our contribution can be summarized as follows:</p><p>(1) We propose a new oversampling method, GAN-FS, to solve the class imbalance problem in intrusion detection. We construct an attack generation model based on WGAN-GP to generate attack samples. (e data are then feature-selected using ANOVA to obtain a rebalanced low-dimensional dataset for training the intrusion detection model. We have modified this in our contribution as follows.</p><p>(2) Based on three popular intrusion detection datasets, we conducted experiments on several machine learning detection models. (e experimental results show that our approach can effectively improve intrusion detection models' performance. Moreover, compared with multiple popular methods, our approach achieves better results. (3) We discuss and analyze the impact of our approach on different datasets and different machine learning detection models.</p><p>(e remainder of the paper is organized as follows. In Section 2, we provide an overview of the relevant studies. Section 3 presents the GAN-FS. (e design, execution, and results are presented in Section 4. Finally, the paper is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Based on GAN and feature selection, we propose an oversampling technique applied to intrusion detection. (erefore, we discuss related work in the following four approaches: intrusion detection method (Section 2.1), feature selection (Section 2.2), oversampling technique (Section 2.3), and generative adversarial network (Section 2.4), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Intrusion Detection.</head><p>As an essential tool of cyber security, IDSs are responsible for identifying and warning of cyber attacks. Since the first paper on IDSs <ref type="bibr" target="#b9">[10]</ref> was published, there have been numerous research achievements in this field. In recent years, IDS has developed rapidly with the help of machine learning.</p><p>Aslahi-Shahri et al. proposed an intrusion detection algorithm based on a genetic algorithm and support vector machine <ref type="bibr" target="#b10">[11]</ref>. (ey used a hybrid algorithm for feature selection and then ranked the selected features according to their importance and finally achieved good results. Elbasiony et al. proposed a hybrid network intrusion detection framework based on random forest and weighted k-means <ref type="bibr" target="#b11">[12]</ref>. (ey combined random forest and k-means based intrusion detection models to construct a hybrid intrusion detection model, which effectively reduces the false alarm rate. Compared with traditional intrusion detection methods, Wang et al. proposed an intrusion detection method based on artificial neural networks and fuzzy clustering <ref type="bibr" target="#b12">[13]</ref>. (e above methods can effectively detect and prevent network attacks, but their research focuses on improving and combining existing methods, ignoring the intrusion detection dataset's imbalance. For example, in the widely used NSL-KDD dataset <ref type="bibr" target="#b13">[14]</ref>, the number of normal samples is 67,343, while the number of R2L and U2R attacks is only 995 and 52, respectively. In the UNSW-NB15 dataset <ref type="bibr" target="#b14">[15]</ref>, the number of normal samples is 37,000, while the number of Shellcode and Worms attacks is only 378 and 44. (e imbalance in the intrusion detection dataset affects the detection accuracy and stability of IDSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Application of Feature Selection in Intrusion Detection.</head><p>Feature selection is referred to as obtaining a subset from an original feature set according to a particular feature selection criterion, selecting the dataset's relevant features. In the field of intrusion detection, the datasets used are characterized by large numbers and high dimensionality. Feature selection reduces the computational difficulty and eliminates data redundancy, thus improving the detection rate of machine learning techniques and reducing false alarms <ref type="bibr" target="#b0">[1]</ref>.</p><p>Khammassi and Krichen propose a GA-LR wrapper approach for feature selection in network intrusion detection <ref type="bibr" target="#b15">[16]</ref>. (ey used a genetic algorithm-based packing method as a search strategy and logistic regression as a learning algorithm to select the best subset of features. Moreover, the method effectively improves intrusion detection performance. <ref type="bibr">Mohammadi et al.</ref> propose an intrusion detection method based on feature selection and clustering algorithm using filter and wrapper methods <ref type="bibr" target="#b16">[17]</ref>. (e filter and wrapper methods are named feature grouping based on linear correlation coefficient algorithm and cuttlefish algorithm. Based on this method, the performance of its intrusion detection model has been significantly improved.</p><p>In general, feature selection is an effective method for data dimensionality reduction and is widely used in intrusion detection. (e feature selection-based data dimensionality reduction method can effectively improve intrusion detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Oversampling Techniques.</head><p>To improve the ability of machine learning models to judge and analyze minority samples in the presence of sample imbalance, researchers have proposed several rebalancing techniques at different levels, such as data-level and algorithm-level. (e data-level oversampling technique increases the number of minority samples by artificial means to improve the dataset's balance. (e typical methods mainly include random oversampling, SMOTE, and ADASYN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In recent years, researchers have proposed new oversampling methods, such as K-means SMOTE and G-SMOTE <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. (ese techniques have improved the data imbalance to varying degrees. Table <ref type="table" target="#tab_0">1</ref> demonstrates the design rationale for the above methods.</p><p>Random oversampling increases the number of minority samples but can lead to severe overfitting. Also, if the minority sample is biased or noisy, this will increase the interference with the classifier. SMOTE uses interpolation to generate samples, avoiding sample overlap due to random oversampling. However, because SMOTE treats all minority samples equally and does not consider the category information of neighbor samples, it cannot effectively enhance decision boundaries, resulting in poor classification results. ADASYN takes into account information about the distribution of the data while generating samples by interpolation. However, network traffic complexity leads to a blurring of its category boundaries, and using this strategy may exacerbate the confusion of decision boundaries.</p><p>In order to solve the problems of the above methods, researchers have made continuous attempts. For few classes, K-means SMOTE generates a different number of samples based on the clustering density. Furthermore, the method does not consider the category labels, thus ensuring that the generated samples are in a safe region. G-SMOTE substitutes the data generation mechanism by defining a flexible geometric region around each minority sample. (en synthetic instances are generated inside the boundaries of the region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Generative Adversarial Networks.</head><p>GAN is a deep learning model that models complex high-dimensional distributions of real-world data. Inspired by the two-person zero-sum game in game theory, it consists of a Generator (G) and a Discriminator (D). G and D are both neural networks. G captures the potential distribution of real data samples and generates new data samples; D is a binary classifier, judging whether the input is real data or generated samples. Classification results will be passed back to G and D through the loss of weight updates. Both networks are trained until D can no longer distinguish real samples from generated samples. (e optimization process is a minimax game problem. (e optimization goal is to achieve a Nash equilibrium so that the generated network can estimate data samples' distribution. (e objective function is defined as follows:</p><formula xml:id="formula_0">min G max D V(D, G) � E x∼p data (x) [log D(x)] + E z∼p data (z) [log(1 -D(G(z)))],</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>where P data is defined as the real sample distribution, P G is the sample distribution generated by the generator, P Z (z) is the noise variable distribution, G(z) is defined as a function of mapping noise to data space, and D(x) represents the probability that the sample x is real data rather than a generated sample. To distinguish between real data and generated samples, D(x) should be maximized and D(G(z)) should be minimized. When P G � P data , the objective function obtains the global optimal solution.</p><p>In the security field, some researchers have applied GAN in their work. Ring et al. use GAN to generate high-quality data flow <ref type="bibr" target="#b19">[20]</ref>. Rigaki and Garcia can successfully bypass detection by using malicious traffic generated by GAN <ref type="bibr" target="#b20">[21]</ref>. Lee and Park proposed an intrusion detection method based on generative adversarial networks and random forests. (ey oversampled the intrusion detection dataset and then used the random forest for classification. (e method achieved better performance on the CICIDS-2017 dataset compared to the original random forest method. However, their work did not consider the instability of GAN and the high dimensionality of the data and did not perform more validation on other datasets and models <ref type="bibr" target="#b21">[22]</ref>. Yilmaz et al. proposed an intrusion detection method based on GAN and MLP <ref type="bibr" target="#b22">[23]</ref>. (ey generated three types of attacks on the UGR 16 dataset to balance the dataset. (e experimental results show that GAN's balanced attack sample dataset produces more accurate results than the unbalanced attack sample set. Vu and Nguyen proposed a method based on Auxiliary Classifier Generative Adversarial Network (ACGAN) to enhance the balance of the dataset <ref type="bibr" target="#b23">[24]</ref>. (e method achieved better performance than machine learning algorithms trained on the original dataset and other sampling techniques. Yin et al. proposed a framework for intrusion detection based on generative adversarial networks. (e generative model in their framework is used to generate other complementary labeled samples for adversarial training, which helps the classifier perform classification <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this paper, we propose a new oversampling method, GAN-FS. Compared with existing work, we design the GAN-FS oversampling method in terms of both the number and dimensionality of the data. We build an attack generation model based on WGAN-GP to generate higher quality samples. And, we introduce ANOVA for feature selection from the perspective of data high dimensionality to further reduce the learning difficulty of the classifier. We also Security and Communication Networks perform experimental validation on several popular datasets and detection models and analyze the impact of different datasets and different machine learning detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Oversampling Technique GAN-FS</head><p>(e imbalance of data affects the performance of machine learning-based IDSs. Due to the lack of analysis on the correlation of features, existing oversampling technology cannot effectively generate high-dimensional network traffic. We build an attack generation model based on WGAN-GP to generate higher quality samples. And, we introduce ANOVA for feature selection from the perspective of data high dimensionality to further reduce the learning difficulty of the classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preprocessing.</head><p>(e dataset used in the context of intrusion detection contains different forms of features such as continuous, discrete, and symbolic with varying resolution and ranges. We need to process the data to make it suitable for our model. (e preprocessing includes numeralization and normalization. We also need to partition the dataset if it does not provide a defined training and testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Numeralization. Intrusion detection data usually</head><p>contains nonnumeric features such as protocols and states.</p><p>(ese nonnumeric features need to be converted to numeric features to suit our model. Nonnumeric features are mapped to integer values between 0 and S-1, where S is the number of symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Normalization.</head><p>(e inconsistent feature scales of data in different dimensions will affect the results of intrusion detection. We need to normalize the data to eliminate the dimension influence between indicators. We scale all the features to [0, 1] except the attack type label. (e min-max normalization is used to scale the data values linearly, as follows:</p><formula xml:id="formula_2">x′ � x -x min x max -x min , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where x is the value before normalization, x ′ is the value after normalization, x max is the maximum value of sample data, and x min is the minimum value of sample data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Partition and Rare Class Attack Oversampling.</head><p>Attack data is a rare class of data in training set in the field of intrusion detection. We need to take rare class data from the training set and oversample them. Before generating samples, we train a generative model to model the distribution of attack data.</p><p>In GAN, the Jensen-Shannon divergence is used to measure the difference between two distributions, but it requires some overlap between the two distributions. When the discriminator is trained to be optimal, there is no overlap between the distribution of the real sample p data and the distribution of the generated sample p G , and two nonoverlapping or negligible overlap distributions cause the generator's gradient to disappear. Wasserstein GAN (WGAN) <ref type="bibr" target="#b25">[26]</ref> effectively improves GAN by increasing the Lipschitz limit and introducing Wasserstein distances. However, WGAN also suffers from the disappearing gradient problem, and WGAN-GP <ref type="bibr" target="#b26">[27]</ref> introduces the gradient penalty to solve this problem. (e structure of the WGAN-GP is the same as that of the GAN, as shown in Figure <ref type="figure">2</ref>.</p><p>(e WGAN-GP objective function is expressed as follows: </p><formula xml:id="formula_4">L � E 􏽥 x ∼ P G [D(􏽥 x)] -E x ∼ P data [D(x)] + λ E 􏽢 x ∼ P 􏽢 x ∇ 􏽢 x D(􏽢 x) � � � � � � � � 2 -1 􏼐 􏼑 2 􏼔 􏼕,<label>(3)</label></formula><p>where P data is the data distribution, P G is the model distribution implicitly defined by 􏽥 x � G(z), z ∼ p(z) (the input z to the generator is sampled from some simple noise distribution, such as the uniform distribution or a spherical Gaussian distribution), and P 􏽢 x defines the uniform sampling along a straight line between the point pairs sampled from the data distribution P data and the generated distribution P G . A penalty on the gradient norm is enforced for random samples 􏽢</p><p>x ∼ P 􏽢 x . In this way, the generator and frequency discriminator can be improved at the same speed to avoid mode-collapse, which leads to the optimization of training effect and the weight of neural network for the poor and improves WGAN's training to a certain extent.</p><p>In the process of generating the sample, noise and rare class attacks are used to train WGAN-GP.  Random noise Real attack traffic Generator Simulated sample Discriminator loss Discriminator Generator loss Simulated Real Figure 2: (e architecture of WGAN-GP.</p><p>Security and Communication Networks 5 2037, 2021, 1, Downloaded from <ref type="url" target="https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059">https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059</ref> by Algeria Hinari NPL, Wiley Online Library on [27/09/2025]. See the Terms and Conditions (<ref type="url" target="https://onlinelibrary.wiley.com/terms-and-conditions">https://onlinelibrary.wiley.com/terms-and-conditions</ref>) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 3.3. Feature Selection. Feature selection is a data dimensionality reduction method and is often used to deal with high-dimensional and complex data. Feature selection is referred to the process of obtaining a subset from an original feature set according to a particular feature selection criterion, which selects the relevant features of the dataset [28]. Feature selection is a process of selecting n most valuable features from the m existing original features to reduce the dimensionality of the dataset. (e filter method is a common feature selection method, and the selection of features is separate from any machine learning technique. It selects features based on scores in various statistical tests and on indicators of relevance. ANOVA (analysis of variance) F-test is a commonly used method of feature selection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. It uses the F-test to determine whether the means of some groups are different and to test statistically whether the means are equal. More specifically, for each feature x i , we assume that x i has the same mean value in the positive and negative category samples, i.e., H 0 : μ S+ � μ S-, where μ denotes the mean value, S+ denotes the set of x i values belonging to the positive category samples, and Sdenotes the set of x i values belonging to the negative category samples; then, the f value is calculated according to the following equation:</p><formula xml:id="formula_5">f_value � S A /(r -1) S E /(n -r) ,<label>(4)</label></formula><p>where S A and S E denote the component and intragroup deviation, respectively, n is the total number of samples, and r is the number of categories, where r is 2. f_value of each feature is calculated separately according to the above steps. Finally, the optimal subset is obtained by ranking the features according to their importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Train and</head><p>Test Machine Learning Model. (e imbalanced dataset affects the machine learning-based intrusion detection model's analysis capability, making its classification results biased towards normal activities and leading to a high false alarm and missed alarm rate. We oversample rare classes of attacks in the training set based on WGAN-GP and then downsample the training set using the ANOVA feature selection method to obtain a low-dimensional rebalanced training set finally. In this step, we use the rebalanced lowdimensional dataset to train the machine learning model. When the model training is completed, we use a test set based on a subset of features to test its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section, we systematically design and conduct a series of experiments and analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Research Questions</head><p>(1) Q1: can our proposed method effectively improve the detection performance of machine learning models?</p><p>(2) Q2: is the combination of GAN and feature selection effective? (3) Q3: is our proposed method better than other oversampling methods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>NSL-KDD: the earliest IDSs dataset was created by the Defense Advanced Research Projects Agency in 1998 and was named the DARPA 1998 dataset. Subsequently, the KDD99 dataset was created from the DARPA 1998 dataset and has become one of the most widely used datasets <ref type="bibr" target="#b30">[31]</ref>. (e presence of many duplicate instances in the KDD99 dataset can affect the detection performance of machine learning methods by biasing them towards normal instances.</p><p>Tavallaee et al. built the NSL-KDD dataset in 2009 based on the KDD99 dataset to solve the above problem by eliminating duplicate records [14]. (e NSL-KDD training set consists of 125,973 records, and the test dataset contains 22,544 records. (e NSL-KDD dataset includes four types of attacks and 41 attributes. (e four types of attacks are DoS, Probe, R2L, and U2R. However, the number of attack instances in this dataset is much lower than normal instances, with only 995 and 52 for R2L and U2R attacks, respectively. UNSW-NB15: in recent years, the Cyber Range Lab of the Australian Centre for Cyber Security has created the UNSW-NB15 dataset. (is dataset contains a variety of novel attacks and is therefore widely used for intrusion detection. (ere are nine types of attacks to simulate the real network environment, namely, Fuzzers, Analysis, Backdoor, DoS, Exploits, Generic, Reconnaissance, Shellcode, and Worms. (e UNSW-NB15 dataset contains a training set and a testing set. (e training set has 82332 records, and the testing set has 175341 records. (e training set is unbalanced, with the number of normal data being much higher than the number of attacks. CICIDS-2017: the dataset was created in a 5-day simulation environment containing network traffic in packet-based and bidirectional stream formats. (e authors extracted more than 80 attributes for each stream and provided additional metadata about IP addresses and attacks. Compared to NSL-KDD and UNSW-NB15, CICIDS-2017 includes a wide range of attack types such as SSH brute force, heartbleed, botnet, DoS, DDoS, web, and penetration attacks. Moreover, with nearly three million data, CICIDS-2017 can evaluate the performance of IDS in large-scale scenarios.</p><p>We increase the number of attacks by oversampling the rare attack categories in the dataset. Tables <ref type="table" target="#tab_4">2</ref><ref type="table" target="#tab_5">3</ref><ref type="table" target="#tab_6">4</ref>show the data distribution of the dataset before and after oversampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Settings.</head><p>In this section, we describe the relevant experimental setup, including the selection of models, the setting of parameters, and the selection of evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Machine Learning Model Selection</head><p>Naive Bayes (NB) is a classification technique based on Bayes' theorem, which assumes that predictors are independent of each other <ref type="bibr" target="#b31">[32]</ref>. Simply put, the Naive Bayes classifier assumes that a feature in a category is independent of the presence of other features. Naive Bayes models are easy to build, and, in addition to being simple, Naive Bayes outperforms even highly complex classification methods. Decision Tree (DT) is widely used in intrusion detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. A decision tree is a tree-like structure with leaves, which represent classifications, and branches, representing the combination of features that lead to those classifications. An example is the classification of nodes of a decision tree by testing their feature values against each other. Moreover, in the work of Mishra et al., decision trees are the single classifier with the best performance <ref type="bibr" target="#b34">[35]</ref>.</p><p>Random Forest (RF) is commonly used as an integration algorithm <ref type="bibr" target="#b35">[36]</ref>. (e integration of classifiers provides a more robust generalization capability than a single base learner and is also widely used in intrusion detection <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. RF integrates multiple weak classifiers, and the final result is obtained by voting or taking the mean. (is allows the overall model to have a high degree of accuracy and generalization. Gradient Boosting Decision Tree (GBDT) is a robust integrated learning algorithm that extends and augments a categorical regression tree model based on gradient augmentation <ref type="bibr" target="#b39">[40]</ref>. (e GBDT iteratively constructs decision trees, and in each iteration, a decision tree is trained from the residuals of the previous tree. (e final result is then obtained cumulatively from the predictions of all trees. Support Vector Machine (SVM) is also one of the most widely used machine learning algorithms <ref type="bibr" target="#b30">[31]</ref>. SVM is a supervised learning method <ref type="bibr" target="#b40">[41]</ref>. It performs classification by constructing an N-dimensional hyperplane that optimally classifies the data into different classes. K-Nearest Neighbors (K-NN) is a data mining algorithm that is theoretically mature and less complex <ref type="bibr" target="#b41">[42]</ref>. (e basic idea is that, in the sample space, if most of the nearest neighbor samples belong to a class, then the samples belong to the same class. Artificial Neural Networks (ANN) is a form of distributed computing inspired by biology. It has a strong self-learning capability and is suitable for solving nonlinear problems, so it is also commonly used in intrusion detection <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Model Settings.</head><p>WGAN-GP is implemented through the deep learning framework Kears. Both generator and discriminator are feedforward neural networks. (e learning rate of the generator and discriminator is 0.0001. (e dimension of the noise vector is 100. Furthermore, the weight clipping threshold of discriminator training is set to 0.01. Root Mean Square Error (RMSE) can characterize the degree of fit between the generated samples and the real samples. As shown in Figure <ref type="figure">3</ref>, the model is close to convergence when trained to 150 rounds. (erefore, the training epoch for WGAN-GP is set to 150.</p><p>Scikit-learn is used to implement the feature selection process and the construction of the machine learning models. After feature selection, the feature subsets of the NSL-KDD and UNSW-NB15 datasets are shown in Table <ref type="table" target="#tab_7">5</ref>. For a detailed explanation of the features, see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. (e four machine learning models are also implemented by scikit-learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Evaluation Metrics.</head><p>IDSs are a vital tool to ensure network security. It is necessary not only to identify attacks accurately but also to avoid false alarms. (erefore, we use recall and precision as metrics. Also, we need to consider the overall accuracy, so we use accuracy as a metric. Besides, we introduce F-measure as a metric to fully evaluate the (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments and</head><p>Results. To answer the three questions mentioned earlier, we design three separate sets of experiments and analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Experiment I.</head><p>To investigate GAN-FS's effectiveness (Q1), we will train machine learning models using the original training set and the training set oversampled by GAN-FS, respectively. (e experiments are arranged as follows:</p><p>(1) Train the machine learning models using the dataset that is not oversampled (2) Train the machine learning models using the dataset oversampled by GAN-FS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Results</head><p>. (e comparison results before and after oversampling using GAN-FS are shown in Figures <ref type="figure" target="#fig_4">4</ref><ref type="figure" target="#fig_6">5</ref><ref type="figure" target="#fig_7">6</ref>. (e results show that the performance of the classifiers is improved to different degrees after oversampling using our proposed method. As shown in Figure <ref type="figure" target="#fig_4">4</ref>(a), the Accuracy of each detector is improved on the NSL-KDD dataset. Such results indicate that our method can improve the overall performance of the detectors. In terms of the performance of individual detectors, GBDT shows the best detection performance. Its Accuracy was improved by about 6% to 83.28%. Figure <ref type="figure" target="#fig_5">4(c)</ref> shows that the Recall of all detectors also improves to varying degrees, which indicates that the samples we generated improve the diversity of attack samples and thus enhance the generalization of the knowledge learning of the detectors. (e improved Recall indicates that the classifier   (is is also because while the detector's generalization ability improves after adding the generated samples, it misclassifies some normal samples as attack samples. Since different detectors have different learning abilities, there is some difference in the degree of impact. Finally, the F-measure of the detectors shows that the overall performance of the detectors is effectively improved. (e same conclusion can be drawn from the experiments on the UNSW-NB15 and CICIDS datasets. Figures <ref type="figure" target="#fig_6">5(a</ref>) and 5(a) show that our approach can effectively improve the Accuracy of the classifier and more significantly improve the Recall of the classifier. Ultimately, the F-measure of each classifier has different degrees of improvement. Notably, in the experiments of CICIDS-2017, the F-measure of GBDT without oversampling has been as high as 99.36%. In this case, our method does not allow further improvement of the classifier, but the impact is also tiny.</p><p>In addition, analyzing the performance of individual classifiers, RF and GBDT achieved quite good performance after oversampling. GBDT showed optimal performance on both NSL-KDD and UNSW-NB15 datasets and showed better performance on the CICIDS-2017 dataset. RF showed optimal performance on the CICIDS-2017 dataset with a 99.6% F-measure. (is demonstrates the powerful generalization ability of RF and GBDT as integrated learning and highlights the effectiveness of our proposed oversampling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Experiment II.</head><p>GAN-FS is based on GAN and feature selection oversampling methods. (is experiment is to verify the effectiveness of the combination of GAN and feature selection. In this experiment, we will process the training set using GAN and feature selection separately, evaluate it using machine learning models, and then compare it with our method. (e experiments are arranged as follows:</p><p>(1) Oversampling the dataset using WGAN-GP (w/o ANOVA)</p><p>(2) Reducing the dimensionality of the dataset using ANOVA (w/o WGAN-GP)</p><p>(3) Oversampling the dataset using GAN-FS 4.4.4. Results. Figures <ref type="figure" target="#fig_8">7</ref><ref type="figure">8</ref><ref type="figure" target="#fig_9">9</ref>show the results of using WGAN-GP and ANOVA and GAN-FS alone. (e experimental results show that the combination of WGAN-GP and ANOVA is effective. GAN and feature selection can improve the detection performance of machine learning models to some extent. However, from the comparison in Figure <ref type="figure" target="#fig_8">7</ref>, we can see that the overall performance of using only WGAN-GP for oversampling or only ANOVA for dimensionality reduction is lower than that of GAN-FS. We can draw the same conclusion from the comparison in Figure <ref type="figure">8</ref>. (is is because WGAN-GP can learn the distribution of the attack samples</p><p>0.7670 0.7823 0.7690 0.7646 0.7554 0.7758 0.7726 0.8052 0.8092 0.8293 0.8328 0.7555 0.7985 0.7924 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value Before After (a) NB DT RF GBDT SVM K-NN ANN 0.5 0.6 0.7 0.8 0.9 1 Value 0.9659 0.9035 0.9682 0.9132 0.9654 0.9732 0.9595 0.9661 0.9646 0.9839 0.9825 0.9833 0.9599 0.9161 Before After (b) NB DT RF GBDT SVM K-NN ANN 0.5 0.6 0.7 0.8 0.9 1 Value 0.6122 0.6915 0.6144 0.6817 0.7422 0.7693 0.7756 0.5915 0.6628 0.6232 0.6742 0.6270 0.6993 0.6482 Before After (c) NB DT RF GBDT SVM K-NN ANN 0.5 0.6 0.7 0.8 0.9 1 Value 0.7495 0.7993 0.7918 0.7599 0.7931 0.7584 0.7921 0.7336 0.8669 0.7582 0.8635 0.7517 0.8451 0.7834 Before After (d) Security and Communication Networks and thus generate samples. (e generated samples can increase the diversity of attack samples, enhance the learning of attack features by the detector, and improve the detection performance. However, the high dimensionality of the original data also affects the analytical performance of the detector, and the use of feature selection can delete some of the features that are not important for analytic learning, thus improving the classifier's performance. However, when the detector performance is high, the improvement from feature selection is subtle. As shown in Figure <ref type="figure" target="#fig_9">9</ref>, the lead of our method is more subtle. (is is mainly because the detector already achieves very high performance when feature selection is not performed. Performing feature selection, in this case, destroys the original combination of features and does not result in a very significant improvement in the detector's performance. Combining the above results, the combination of WGAN-GP and ANOVA, i.e., GAN-FS, is compelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Experiment III.</head><p>As we talked about in Section 2, SMOTE <ref type="bibr" target="#b2">[3]</ref> and ADASYN <ref type="bibr" target="#b3">[4]</ref> are two classical oversampling methods, while K-means SMOTE <ref type="bibr" target="#b18">[19]</ref> and G-SMOTE <ref type="bibr" target="#b17">[18]</ref> are two newer methods proposed in recent years. We will compare these four methods. In addition, we also compare them with the GAN-based methods of Vu and Nguyen <ref type="bibr" target="#b23">[24]</ref> and Lee and Park <ref type="bibr" target="#b21">[22]</ref>. (e F-measure is an overall evaluation of the Precision and Recall, which we use to measure the methods' performance. (e experiments are arranged as follows:</p><p>(1) Oversampling the dataset using baseline separately (2) Oversampling the dataset using GAN-FS 4.4.6. Results. (e experimental results compared to the baseline are shown in Tables <ref type="table" target="#tab_13">6</ref><ref type="table" target="#tab_14">7</ref><ref type="table" target="#tab_15">8</ref>. From the table, we can see that the detectors' performance based on our proposed method tends to be higher than the other baselines compared to the other methods. From the perspective of individual detectors, we can see that our method performs better on DT, RF, GBDT, and ANN. (is is related to the principle of the detector. (e idea of the tree model is to construct a tree with the fastest decreasing entropy using information entropy as a metric. Our method generates samples based on the original distribution, which increases the diversity of samples and removes unnecessary features using feature selection. (erefore, after oversampling, the decision tree can better classify the samples. Compared with DT, RF and GBDT are integrated learning algorithms with more excellent learning capability and can improve classification performance. (e experimental results also show that the performance of RF and GBDT are generally higher than DT. As a neural network structure, ANN is also a model with higher learning ability, and its performance is improved by increasing the number of attack samples.</p><p>However, due to different algorithm principles, there are differences in the performance of the classifiers after oversampling. For the K-NN model, we find that the advantage</p><p>0.7278 0.8303 0.9278 0.8835 0.8824 0.9032 0.7306 0.8666 0.9205 0.8857 0.8875 0.8877 0.7035 0.7121 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value Before After (a) 0.9267 0.7110 0.9865 0.9530 0.9956 0.9711 0.9830 0.9164 0.9203 0.9147 0.9814 0.9396 0.9307 0.8972 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value Before After (b) 0.6516 0.6517 0.6895 0.9022 0.8463 0.8893 0.8449 0.9860 0.8217 0.9718 0.8182 0.9024 0.6069 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value Before After (c) 0.9270 0.9080 0.8931 0.9433 0.8682 0.9490 0.9081 0.9137 0.9110 0.9162 0.7065 0.7550 0.7652 0.7541 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value Before After (d) 10 Security and Communication Networks of our method is not very prominent. (is is because traditional oversampling methods such as SMOTE are based on K-NN to generate samples. Generating samples based on this method and then training the K-NN algorithm can improve the classifier's performance. In addition, although our method improves the performance of the SVM, in the experimental results of NSL-KDD (Table <ref type="table" target="#tab_13">6</ref>), our method is not optimal. (is is because SVM solves the maximum partition hyperplane using support vectors. When the original SVM classification is not optimal, generating samples in a safe geometric space (G-SMOTE) will optimize the classification hyperplane more effectively. Finally, as shown in Figure <ref type="figure" target="#fig_0">10</ref>, we count the number of times that different methods achieve the optimal performance. (e detectors trained based on our method achieved 12 optimal Accuracy and 14 optimal F-measure,</p><p>0.6857 0.9667 0.9299 0.9819 0.9850 0.9962 0.9975 0.9984 0.9707 0.977 0.9737 0.7239 0.5 0.4 0.3 0.6 0.7 0.8 0.9 1 NB DT RF GBDT K-NN ANN Value Before After (a) 0.3433 0.8837 0.7669 0.9405 0.9550 0.9905 0.9959 0.9957 0.9983 0.9710 0.9625 0.3818 0.5 0.4 0.3 0.6 0.7 0.8 0.9 1 Value NB DT RF GBDT K-NN ANN Before After (b) 0.6518 0.9571 0.9252 0.9697 0.9696 0.9900 0.9914 0.9963 0.8529 0.9102 0.9018 0.6482 0.5 0.4 0.3 0.6 0.7 0.8 0.9 1 Value NB DT RF GBDT K-NN ANN Before After (c) 0.4498 0.9189 0.8387 0.9549 0.9622 0.9902 0.9936 0.9960 0.9198 0.9397 0.9311 0.4806 0.5 0.4 0.3 0.6 0.7 0.8 0.9 1 Value NB DT RF GBDT K-NN ANN Before After (d) 0.7833 0.7924 0.7745 0.7734 0.7985 0.8137 0.7757 0.7555 0.7983 0.7451 0.8328 0.7813 0.7661 0.8293 0.7716 0.7779 0.8092 0.8014 0.7704 0.8052 0.7744 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value w/o ANOVA w/o WGAN-GP GAN-FS (a) 0.7931 0.7686 0.7603 0.7921 0.8097 0.7598 0.7918 0.7886 0.7285 0.8669 0.7688 0.7596 0.8635 0.7552 0.7636 0.8451 0.7942 0.7547 0.7993 0.7574 0.7691 w/o ANOVA w/o WGAN-GP GAN-FS 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value (b) Security and Communication Networks 11 2037, 2021, 1, Downloaded from <ref type="url" target="https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059">https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059</ref> by Algeria Hinari NPL, Wiley Online Library on [27/09/2025]. See the Terms and Conditions (<ref type="url" target="https://onlinelibrary.wiley.com/terms-and-conditions">https://onlinelibrary.wiley.com/terms-and-conditions</ref>) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 0.7217 0.9032 0.7463 0.8657 0.8824 0.8574 0.8666 0.9205 0.9006 0.8361 0.9278 0.8916 0.8963 0.8857 0.8659 0.8906 0.8877 0.8516 0.8857 0.7121 0.7324 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value w/o ANOVA w/o WGAN-GP GAN-FS (a) 0.7612 0.927 0.8922 0.908 0.8869 0.8931 0.9433 0.931 0.8738 0.949 0.9157 0.9196 0.9137 0.8937 0.9138 0.9162 0.8824 0.9108 0.755 0.7696 0.7736 0.5 0.6 0.7 0.8 0.9 1 NB DT RF GBDT SVM K-NN ANN Value w/o ANOVA w/o WGAN-GP GAN-FS (b) Figure 8: Results of Experiment II on UNSW-NB15 dataset. (a) Accuracy. (b) F-measure. 0.7221 0.9132 0.9667 0.958 0.9819 0.9566 0.9828 0.9962 0.9779 0.9961 0.9984 0.9687 0.9979 0.977 0.9766 0.9764 0.7239 0.719 0.5 0.4 0.6 0.7 0.8 0.9 1 NB DT RF GBDT K-NN ANN Value w/o ANOVA w/o WGAN-GP GAN-FS (a) 0.9189 0.8911 0.8172 0.9549 0.8860 0.9569 0.9902 0.9415 0.9901 0.9960 0.9138 0.9946 0.9397 0.9382 0.9380 0.4806 0.4720 0.4789 0.5 0.4 0.6 0.7 0.8 0.9 1 NB DT RF GBDT K-NN ANN Value w/o ANOVA w/o WGAN-GP GAN-FS (b)  From these results, we can see that our method can better improve the detection performance of the intrusion detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we take the perspective of imbalance and high dimensionality of datasets in intrusion detection and propose an oversampling intrusion detection technique based on GAN and feature selection. For one thing, our approach proposes to focus on oversampling the rare classes of attack samples in order to improve the effectiveness of intrusion detection. For another thing, we concentrate on only imperative features of attack samples using the ANOVA feature selection method. (en, the obtained lowdimensional rebalanced dataset is used to train intrusion detection classifiers. Experimental results show that our approach improves the performance of detecting intrusion detection models and outperforms other baselines.</p><p>As for future work, we first plan to explore the conjunction between our approach and deep learning. In addition, we will try to assign different weights to features to better reflect the significance of each feature in classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>illustrates the workflow of GAN-FS. (ere are five steps in the framework: Data Preprocessing, Data Partition, Rare class Oversampling, Feature Selection, and Train &amp; Test ML Model. (i) Step 1: the dataset is preprocessed and then divided into a training set and a testing set. (ii) Step 2: the training set is divided into rare class data and other class data by data partitioning. (iii) Step 3: the GAN model uses rare class data to generate samples. (iv) Step 4: the oversampled data is combined with the other class data obtained in step 2, and then, feature selection is performed. (e optimal feature subset and the corresponding new low-dimensional training set are obtained in the feature selection step. (v) Step 5: finally, the new training set is used to train the machine learning (ML) model, and the testing set is used to test the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>(e training process begins with fixing the discriminator and training the generator to model the distribution of real data. When the discriminator cannot correctly distinguish whether the samples are coming from the real attack set or the generator, fix the generator and start training the discriminator. When the discriminator can correctly distinguish between samples through continuous training, fix the discriminator and the training generator. Follow this process for iterative training, and finally, use the generator to generate attack samples. (e generated attack samples are eventually added to the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: (e workflow of GAN-FS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 RMSEFigure 3 :</head><label>63</label><figDesc>Figure 3: (e convergence curve of WGAN-GP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 (</head><label>4</label><figDesc>b) shows the precision of the detectors. It can be seen from the figure that the precision of individual classifiers has declined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of Experiment I on the NSL-KDD dataset. (a) Accuracy. (b) Precision. (c) Recall. (d) F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of Experiment I on the UNSW-NB15 dataset. (a) Accuracy. (b) Precision. (c) Recall. (d) F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of Experiment I on the CICIDS-2017 dataset. (a) Accuracy. (b) Precision. (c) Recall. (d) F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Results of Experiment II on NSL-KDD dataset. (a) Accuracy. (b) F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Results of Experiment II on CICIDS-2017 dataset. (a) Accuracy. (b) F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Design rationale of different oversampling techniques.</figDesc><table><row><cell>Technique</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Distribution of records in NSL-KDD before and after oversampling.</figDesc><table><row><cell>Type</cell><cell>Before oversampling</cell><cell>After oversampling</cell></row><row><cell>Normal</cell><cell>67343</cell><cell>67343</cell></row><row><cell>DoS</cell><cell>45927</cell><cell>45927</cell></row><row><cell>Probe</cell><cell>11656</cell><cell>11656</cell></row><row><cell>R2L</cell><cell>995</cell><cell>10995</cell></row><row><cell>U2R</cell><cell>52</cell><cell>10052</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Distribution of records in UNSW-NB15 before and after oversampling.</figDesc><table><row><cell>Type</cell><cell>Before oversampling</cell><cell>After oversampling</cell></row><row><cell>Normal</cell><cell>37000</cell><cell>37000</cell></row><row><cell>Generic</cell><cell>18871</cell><cell>18871</cell></row><row><cell>Exploits</cell><cell>11132</cell><cell>11132</cell></row><row><cell>Fuzzers</cell><cell>6062</cell><cell>6062</cell></row><row><cell>DoS</cell><cell>4089</cell><cell>4089</cell></row><row><cell>Reconnaissance</cell><cell>3496</cell><cell>3496</cell></row><row><cell>Analysis</cell><cell>677</cell><cell>10677</cell></row><row><cell>Backdoor</cell><cell>583</cell><cell>10583</cell></row><row><cell>Shellcode</cell><cell>378</cell><cell>10378</cell></row><row><cell>Worms</cell><cell>44</cell><cell>10044</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Distribution of records in CICIDS-2017 before and after oversampling.</figDesc><table><row><cell>Type</cell><cell>Before oversampling</cell><cell>After oversampling</cell></row><row><cell>Normal</cell><cell>1363935</cell><cell>1363935</cell></row><row><cell>DoS</cell><cell>151735</cell><cell>151735</cell></row><row><cell>PortScan</cell><cell>95135</cell><cell>95135</cell></row><row><cell>DDoS</cell><cell>76878</cell><cell>76878</cell></row><row><cell>Patator</cell><cell>8290</cell><cell>8290</cell></row><row><cell>Web attack</cell><cell>1281</cell><cell>11281</cell></row><row><cell>Bot</cell><cell>1163</cell><cell>11163</cell></row><row><cell>Infiltration</cell><cell>20</cell><cell>10020</cell></row><row><cell>Heartbleed</cell><cell>8</cell><cell>10008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Subsets of features selected based on ANOVA.</figDesc><table><row><cell>Dataset</cell><cell>Selected features</cell><cell>Quantity</cell></row><row><cell></cell><cell>protocol_type, flag, logged_in, count, serror_rate, srv_serror_rate, rerror_rate, srv_rerror_rate, same_srv_rate,</cell><cell></cell></row><row><cell>NSL-KDD</cell><cell>dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_serror_rate, dst_host_srv_serror_rate,</cell><cell>16</cell></row><row><cell></cell><cell>dst_host_rerror_rate, dst_host_srv_rerror_rate</cell><cell></cell></row><row><cell>UNSW-NB15</cell><cell>Proto, dttl, dloss, sinpkt, swin, stcpb, dtcpb, dwin, dmean, ct_state_ttl, ct_dst_ltm, ct_src_dport_ltm, is_sm_ips_ports</cell><cell>13</cell></row><row><cell>CICIDS-2017</cell><cell></cell><cell>39</cell></row><row><cell></cell><cell>Init_Win_bytes_backward, Idle Mean, Idle Std, Idle Max, Idle Min</cell><cell></cell></row></table><note><p>Destination Port, Flow Duration, Fwd Packet Length Max, Fwd Packet Length Min, Fwd Packet Length Mean, Bwd Packet Length Max, Bwd Packet Length Min, Bwd Packet Length Mean, Bwd Packet Length Std, Flow Packets/s, Flow IAT Mean, Flow IAT Std, Flow IAT Max, Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Bwd IAT Std, Bwd IAT Max, Fwd PSH Flags, Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance, FIN Flag Count, SYN Flag Count, PSH Flag Count, ACK Flag Count, URG Flag Count, Down/Up Ratio, Average Packet Size, Avg Fwd Segment Size, Avg Bwd Segment Size,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Performance of different methods on the NSL-KDD dataset. Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059 by Algeria Hinari NPL, Wiley Online Library on [27/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License respectively, which are much higher than other methods.</figDesc><table><row><cell></cell><cell></cell><cell>SMOTE</cell><cell>ADASYN</cell><cell>K-SMOTE</cell><cell>G-SMOTE</cell><cell>ACGAN-SVM</cell><cell>GAN</cell><cell>Proposed</cell></row><row><cell>NB</cell><cell>Acc F1</cell><cell>0.8475 0.8552</cell><cell>0.8090 0.8132</cell><cell>0.8246 0.8307</cell><cell>0.8027 0.7973</cell><cell>0.7595 0.7395</cell><cell>0.7771 0.7600</cell><cell>0.8052 0.7993</cell></row><row><cell>DT</cell><cell>Acc F1</cell><cell>0.7856 0.7746</cell><cell>0.7583 0.7392</cell><cell>0.7736 0.7624</cell><cell>0.8101 0.8047</cell><cell>0.7983 0.7946</cell><cell>0.7646 0.7564</cell><cell>0.8092 0.8451</cell></row><row><cell>RF</cell><cell>Acc F1</cell><cell>0.7571 0.7363</cell><cell>0.7494 0.7249</cell><cell>0.7837 0.7715</cell><cell>0.7822 0.7694</cell><cell>0.7571 0.7365</cell><cell>0.7618 0.7424</cell><cell>0.8293 0.8635</cell></row><row><cell>GBDT</cell><cell>Acc F1</cell><cell>0.7800 0.7751</cell><cell>0.7780 0.7646</cell><cell>0.7840 0.7774</cell><cell>0.7749 0.7684</cell><cell>0.7741 0.7686</cell><cell>0.7785 0.7737</cell><cell>0.8328 0.8669</cell></row><row><cell>SVM</cell><cell>Acc F1</cell><cell>0.8080 0.8023</cell><cell>0.8025 0.7963</cell><cell>0.7821 0.7686</cell><cell>0.8407 0.8425</cell><cell>0.7781 0.7632</cell><cell>0.7798 0.7654</cell><cell>0.7555 0.7918</cell></row><row><cell>K-NN</cell><cell>Acc F1</cell><cell>0.7924 0.7823</cell><cell>0.7890 0.7812</cell><cell>0.7921 0.7821</cell><cell>0.7921 0.7835</cell><cell>0.7758 0.7599</cell><cell>0.7758 0.7599</cell><cell>0.7985 0.7921</cell></row><row><cell>ANN</cell><cell>Acc F1</cell><cell>0.7661 0.7577</cell><cell>0.7850 0.7865</cell><cell>0.7802 0.7759</cell><cell>0.7850 0.7814</cell><cell>0.7625 0.7455</cell><cell>0.7587 0.7377</cell><cell>0.7924 0.7931</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Performance of different methods on the UNSW-NB15 dataset.</figDesc><table><row><cell></cell><cell></cell><cell>SMOTE</cell><cell>ADASYN</cell><cell>K-SMOTE</cell><cell>G-SMOTE</cell><cell>ACGAN-SVM</cell><cell>GAN</cell><cell>Proposed</cell></row><row><cell>NB</cell><cell>Acc F1</cell><cell>0.7241 0.7627</cell><cell>0.7378 0.7719</cell><cell>0.7265 0.7643</cell><cell>0.7228 0.7619</cell><cell>0.7246 0.7628</cell><cell>0.7243 0.7629</cell><cell>0.7121 0.7550</cell></row><row><cell>DT</cell><cell>Acc F1</cell><cell>0.8904 0.9156</cell><cell>0.8837 0.9110</cell><cell>0.6534 0.6670</cell><cell>0.8854 0.9112</cell><cell>0.8779 0.9040</cell><cell>0.7269 0.7569</cell><cell>0.8877 0.9162</cell></row><row><cell>RF</cell><cell>Acc F1</cell><cell>0.9077 0.9295</cell><cell>0.8993 0.9229</cell><cell>0.8820 0.9090</cell><cell>0.9097 0.9313</cell><cell>0.8872 0.9109</cell><cell>0.8629 0.8896</cell><cell>0.8857 0.9137</cell></row><row><cell>GBDT</cell><cell>Acc F1</cell><cell>0.9086 0.9308</cell><cell>0.9200 0.9408</cell><cell>0.8870 0.9146</cell><cell>0.8997 0.9232</cell><cell>0.8862 0.9101</cell><cell>0.8847 0.9088</cell><cell>0.9278 0.9490</cell></row><row><cell>SVM</cell><cell>Acc F1</cell><cell>0.8937 0.9171</cell><cell>0.9098 0.9314</cell><cell>0.8679 0.8954</cell><cell>0.8993 0.9220</cell><cell>0.8302 0.8682</cell><cell>0.8309 0.8688</cell><cell>0.9205 0.9433</cell></row><row><cell>K-NN</cell><cell>Acc F1</cell><cell>0.8723 0.8983</cell><cell>0.8738 0.8997</cell><cell>0.8744 0.9002</cell><cell>0.8694 0.8956</cell><cell>0.8666 0.8931</cell><cell>0.8666 0.8931</cell><cell>0.8824 0.9080</cell></row><row><cell>ANN</cell><cell>Acc F1</cell><cell>0.8630 0.8893</cell><cell>0.7583 0.7848</cell><cell>0.7397 0.7649</cell><cell>0.8780 0.9028</cell><cell>0.8325 0.8616</cell><cell>0.8712 0.8970</cell><cell>0.9032 0.9270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Performance of different methods on the CICIDS-2017 dataset. Figure 10: Number of times the optimal value is obtained for each method.Security and Communication Networks 13 2037, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059 by Algeria Hinari NPL, Wiley Online Library on [27/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table><row><cell></cell><cell></cell><cell>SMOTE</cell><cell>ADASYN</cell><cell>K-SMOTE</cell><cell>G-SMOTE</cell><cell>ACGAN-SVM</cell><cell>GAN</cell><cell>Proposed</cell></row><row><cell>NB</cell><cell>Acc F1</cell><cell>0.6652 0.4854</cell><cell>0.6816 0.2653</cell><cell>0.7238 0.3105</cell><cell>0.7543 0.4425</cell><cell>0.7263 0.4836</cell><cell>0.7227 0.4797</cell><cell>0.7239 0.4806</cell></row><row><cell>DT</cell><cell>Acc F1</cell><cell>0.9393 0.8220</cell><cell>0.9585 0.8895</cell><cell>0.9459 0.8454</cell><cell>0.2490 0.3414</cell><cell>0.9746 0.9331</cell><cell>0.9721 0.9264</cell><cell>0.9770 0.9397</cell></row><row><cell>RF</cell><cell>Acc F1</cell><cell>0.9969 0.9921</cell><cell>0.9950 0.9872</cell><cell>0.9684 0.9130</cell><cell>0.5847 0.4856</cell><cell>0.9692 0.9154</cell><cell>0.9703 0.9187</cell><cell>0.9984 0.9960</cell></row><row><cell>GBDT</cell><cell>Acc F1</cell><cell>0.9921 0.9801</cell><cell>0.9947 0.9867</cell><cell>0.9928 0.9817</cell><cell>0.9779 0.9468</cell><cell>0.9888 0.9710</cell><cell>0.9719 0.9333</cell><cell>0.9962 0.9902</cell></row><row><cell>K-NN</cell><cell>Acc F1</cell><cell>0.9839 0.9599</cell><cell>0.9823 0.9562</cell><cell>0.9850 0.9622</cell><cell>0.9694 0.9267</cell><cell>0.9847 0.9615</cell><cell>0.9830 0.9613</cell><cell>0.9819 0.9549</cell></row><row><cell>ANN</cell><cell>Acc F1</cell><cell>0.8161 0.6811</cell><cell>0.3498 0.3774</cell><cell>0.8925 0.7153</cell><cell>0.5665 0.4334</cell><cell>0.9294 0.8185</cell><cell>0.9227 0.8311</cell><cell>0.9667 0.9189</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SMOTE, 1 ADASYN, 1</cell><cell></cell><cell>SMOTE, 1 ADASYN, 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>K-SMOTE, 2</cell><cell></cell><cell cols="2">K-SMOTE, 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>G-SMOTE, 2</cell><cell></cell></row><row><cell></cell><cell>Proposed, 12</cell><cell></cell><cell></cell><cell>G-SMOTE, 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Proposed, 14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>2037, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/9947059 by Algeria Hinari NPL, Wiley Online Library on [27/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>(is work was partially supported by the <rs type="funder">National Natural Science of Foundation of China</rs> (nos. <rs type="grantNumber">61902010</rs> and <rs type="grantNumber">61671030</rs>), <rs type="funder">CCF-NSFocus Funding</rs> (no.<rs type="grantNumber">2020003</rs>), <rs type="funder">Beijing Excellent Talent Funding-Youth Project</rs> (no.<rs type="grantNumber">201800 0020124G039</rs>), and Project of <rs type="projectName">Beijing Municipal Education Commission</rs> (no.<rs type="grantNumber">KM202110005025</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6SVKFcx">
					<idno type="grant-number">61902010</idno>
				</org>
				<org type="funding" xml:id="_Z55aXDh">
					<idno type="grant-number">61671030</idno>
				</org>
				<org type="funding" xml:id="_vcZKgDM">
					<idno type="grant-number">2020003</idno>
				</org>
				<org type="funded-project" xml:id="_UnmVB5w">
					<idno type="grant-number">201800 0020124G039</idno>
					<orgName type="project" subtype="full">Beijing Municipal Education Commission</orgName>
				</org>
				<org type="funding" xml:id="_4HNUVuK">
					<idno type="grant-number">KM202110005025</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>We use public datasets, which are deposited in public repositories NSL-KDD (<ref type="url" target="https://www.unb.ca/cic/datasets/nsl.html">https://www.unb.ca/cic/datasets/nsl.  html</ref>), UNSW-NB15 (<ref type="url" target="https://cloudstor.aarnet.edu.au/plus/index.php/s/2DhnLGDdEECo4ys?path=%2FUNSW-NB15%20-%20CSV%20Files">https://cloudstor.aarnet.edu.au/plus/  index.php/s/2DhnLGDdEECo4ys?</ref>path�%2FUNSW-NB15% 20-%20CSV%20Files), and CICIDS-2017 (<ref type="url" target="https://www.unb.ca/cic/datasets/ids-2017.html">https://www.unb.  ca/cic/datasets/ids-2017.html</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>(e authors declare that they have no conflicts of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey of intrusion detection systems: techniques, datasets and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khraisat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamruzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybersecurity</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Machine learning and deep learning methods for intrusion detection systems: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">4396</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Adasyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">2008. 2008. September 2008</date>
		</imprint>
	</monogr>
	<note>Proceedings of the Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue Generation with Gan</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Menlo Park, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">October 2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on feature selection methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An intrusion-detection model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hybrid method consisting of ga and svm for intrusion detection system</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Aslahi-Shahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chizari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hybrid network intrusion detection framework based on random forests and weighted k-means</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Elbasiony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sallam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Eltobely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ain Shams Engineering Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="753" to="762" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new approach to intrusion detection using artificial neural networks and fuzzy clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A detailed analysis of the kdd cup 99 data set</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tavallaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications</title>
		<meeting>the 2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-07">July 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsw-nb15: a comprehensive data set for network intrusion detection systems (unsw-nb15 network data set)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Military Communications and Information Systems Conference (MilCIS)</title>
		<meeting>the Military Communications and Information Systems Conference (MilCIS)<address><addrLine>Canberra, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">2015. November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A ga-lr wrapper approach for feature selection in network intrusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Khammassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krichen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="255" to="277" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cyber intrusion detection by combined feature selection algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mirvaziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazizadeh-Ahsaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karimipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of information security and applications</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="80" to="88" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric smote a geometrically enhanced drop-in replacement for smote</title>
		<author>
			<persName><forename type="first">G</forename><surname>Douzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page" from="118" to="135" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving imbalanced learning through a heuristic oversampling method based on k-means and smote</title>
		<author>
			<persName><forename type="first">G</forename><surname>Douzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Last</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flow-based network traffic generation using generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schlör</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bringing a gan to a knife-fight: adapting malware communication to avoid detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rigaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Security and Privacy Workshops (SPW)</title>
		<meeting>the 2018 IEEE Security and Privacy Workshops (SPW)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gan-based imbalanced data intrusion detection system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Personal and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Addressing imbalanced data problem with generative adversarial network for intrusion detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Masum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI)</title>
		<meeting>the 2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-08">August 2020</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handling imbalanced data in intrusion detection systems using generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">U</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1155/2021/9947059byAlgeriaHinariNPL</idno>
		<idno>27/09/2025</idno>
		<ptr target="https://onlinelibrary.wiley.com/terms-and-conditions" />
	</analytic>
	<monogr>
		<title level="m">See the Terms and Conditions</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>14 Security and Communication Networks 2037, 2021, 1 on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing network intrusion detection classifiers using supervised adversarial training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ce Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the 31st Conference on Advances in Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature selection in machine learning: a new perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effects of machine learning approach in flow-based anomaly detection on software-defined networking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-level intrusion detection method for abnormal network behaviors</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of kdd99 dataset usage in intrusion detection and machine learning between 2010 and 2015</title>
		<author>
			<persName><forename type="first">A</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Preprints</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2016">1954v1, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network intrusion detection using Naive Bayes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Patra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer science and network security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="258" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel hierarchical intrusion detection system based on decision tree and rules-based models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maglaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferrag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Derdour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Janicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS)</title>
		<meeting>the 2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS)<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intrusion detection system based on decision tree over big data in fog environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<idno>ID 4680867</idno>
	</analytic>
	<monogr>
		<title level="m">Wireless Communications and Mobile Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A detailed investigation and analysis of using machine learning techniques for intrusion detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varadharajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tupakula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="686" to="728" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Random forest modeling for network intrusion detection system</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farnaaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jabbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="213" to="217" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards effective network intrusion detection: a hybrid model integrating gini index and gbdt with pso</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sensors</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Article ID 1578314, 9 pages</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Ensemble Methods: Foundations and Algorithms</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A Practical Guide to Support Vector Classification</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Taiwan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Use of k-nearest neighbor classifier for intrusion detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="439" to="448" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance analysis of nsl-kdd dataset using ann</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ingre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Signal Processing and Communication Engineering Systems</title>
		<meeting>the International Conference on Signal Processing and Communication Engineering Systems<address><addrLine>Guntur, India</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-01">January 2015</date>
			<biblScope unit="page" from="92" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
